{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling relative strain prevalence\n",
    "\n",
    "This notebook explores Pyro models for forecasting relative strain prevalance based on GISAID sequence data labeled with Pangolin lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoDelta, AutoNormal, init_to_median\n",
    "from pyro.optim import ClippedAdam\n",
    "from pyrocov import pangolin\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "We'll use all GISAID data. You'll need to request a feed from gisaid.org, download, then run\n",
    "```sh\n",
    "python preprocess_gisaid.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/gisaid.columns.pkl\", \"rb\") as f:\n",
    "    columns = pickle.load(f)\n",
    "print(\"Training on {} rows with columns:\".format(len(columns[\"day\"])))\n",
    "print(\", \".join(columns.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineages = list(map(pangolin.compress, columns[\"lineage\"]))\n",
    "print(f\"Top 12 of {len(set(lineages))} lineages\")\n",
    "print(\"-\" * 30)\n",
    "for lineage, count in Counter(lineages).most_common(12):\n",
    "    print(f\"{count: >10d} {lineage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = columns[\"location\"]\n",
    "print(f\"Top 12 of {len(set(locations))} lineages\")\n",
    "print(\"-\" * 30)\n",
    "for location, count in Counter(locations).most_common(12):\n",
    "    print(f\"{count: >10d} {location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating regions and lineages\n",
    "\n",
    "We'll aggregate rare lineages into their parents, and aggregate locations by either US state, UK regions, or other country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pangolin.merge_lineages(Counter(lineages), min_count=500)\n",
    "lineages = [mapping.get(x, x) for x in lineages]\n",
    "print(len(set(lineages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_data = Counter()\n",
    "location_id = {}\n",
    "lineage_id = {}\n",
    "lineage_id_inv = [None] * len(lineage_id)\n",
    "for day, location, lineage in zip(columns[\"day\"], columns[\"location\"], lineages):\n",
    "    parts = location.split(\"/\")\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    parts = [p.strip() for p in parts[:3]]\n",
    "    if parts[1] not in (\"USA\", \"United Kingdom\"):\n",
    "        parts = parts[:2]\n",
    "    location = \" / \".join(parts)\n",
    "    x = location_id.setdefault(location, len(location_id))\n",
    "    s = lineage_id.setdefault(lineage, len(lineage_id))\n",
    "    t = day // 7\n",
    "    sparse_data[t, x, s] += 1\n",
    "lineage_id_inv = [None] * len(lineage_id)\n",
    "for name, i in lineage_id.items():\n",
    "    lineage_id_inv[i] = name\n",
    "    \n",
    "T = 1 + max(columns[\"day\"]) // 7\n",
    "P = len(location_id)\n",
    "S = len(lineage_id)\n",
    "dense_data = torch.zeros(T, P, S)\n",
    "for (t, p, s), n in sparse_data.items():\n",
    "    dense_data[t, p, s] = n\n",
    "print(dense_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After aggregation we can define edges relating lineages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pangolin.find_edges(list(lineage_id))\n",
    "edges = torch.tensor([[lineage_id[u], lineage_id[v]] for u, v in edges], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Pyro model\n",
    "\n",
    "Our first model is a logistic growth model with a Dirichlet-multinomial likelihood (the multivariate generalization of negative binomial likelihood). This accounts for lineage hierarchy but ignores any spatial structure across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(dense_data, edges):\n",
    "    T, P, S = dense_data.shape\n",
    "    time_plate = pyro.plate(\"time\", T, dim=-2)\n",
    "    place_plate = pyro.plate(\"place\", P, dim=-1)\n",
    "    time = torch.arange(float(T)) * 7 / 365.25  # in years\n",
    "    time -= time.max()\n",
    "    \n",
    "    # Assume relative growth rate depends on strain but not time or place.\n",
    "    log_rate = pyro.sample(\n",
    "        \"log_rate\",\n",
    "        dist.Normal(0, 1).expand([S]).to_event(1).mask(False),\n",
    "    )\n",
    "    # Account for hierarchy by assuming related strains have similar growth rate.\n",
    "    tree_scale = pyro.sample(\"tree_scale\", dist.LogNormal(0, 5))\n",
    "    with pyro.plate(\"edges\", len(edges), dim=-1):\n",
    "        u, v = log_rate[edges].unbind(-1)\n",
    "        pyro.sample(\n",
    "            \"rate_change\",\n",
    "            dist.Laplace(0, tree_scale),\n",
    "            obs=u - v,\n",
    "        )\n",
    "\n",
    "    # Assume places differ only in their initial infection count.\n",
    "    with place_plate:\n",
    "        log_init = pyro.sample(\n",
    "            \"log_init\",\n",
    "            dist.Normal(0, 10).expand([S]).to_event(1),\n",
    "        )\n",
    "\n",
    "    # Finally observe overdispersed counts.\n",
    "    concentration = pyro.sample(\"concentration\", dist.LogNormal(2, 4))\n",
    "    base_rate = (log_init + log_rate * time[:, None, None]).softmax(dim=-1)\n",
    "    with time_plate, place_plate:\n",
    "        pyro.sample(\n",
    "            \"obs\",\n",
    "            dist.DirichletMultinomial(\n",
    "                total_count=dense_data.sum(-1).max(),\n",
    "                concentration=concentration * base_rate,\n",
    "                is_sparse=True,  # uses a faster algorithm\n",
    "            ),\n",
    "            obs=dense_data,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "We'll use simple MAP estimation via Pyro's SVI and an `AutoDelta` guide. I'm defining a custom initialization function to initialize forecasts to uniformly even rather than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_loc_fn(site):\n",
    "    if site[\"name\"] in (\"log_init\", \"log_rate\"):\n",
    "        return torch.zeros(site[\"fn\"].shape())\n",
    "    return init_to_median(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(20210204)\n",
    "\n",
    "num_steps = 501\n",
    "guide = AutoDelta(model, init_loc_fn=init_loc_fn)\n",
    "guide(dense_data, edges)  # Initializes guide so we can count parameters.\n",
    "print(\"Traning guide with {} parameters:\".format(sum(p.numel() for p in guide.parameters())))\n",
    "optim = ClippedAdam({\"lr\": 0.05, \"lrd\": 0.1 ** (1 / num_steps)})\n",
    "svi = SVI(model, guide, optim, Trace_ELBO())\n",
    "losses = []\n",
    "num_obs = dense_data.count_nonzero()\n",
    "for step in range(num_steps):\n",
    "    loss = svi.step(dense_data, edges) / num_obs\n",
    "    losses.append(loss)\n",
    "    if step % 50 == 0:\n",
    "        median = guide.median()\n",
    "        concentration = median[\"concentration\"].item()\n",
    "        tree_scale = median[\"tree_scale\"].item()\n",
    "        print(f\"step {step: >4d} loss = {loss:0.3g}\\t\"\n",
    "              f\"conc. = {concentration:0.3g}\\ttree_scale = {tree_scale:0.3g}\")\n",
    "        \n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"SVI step\")\n",
    "plt.ylabel(\"loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we can extract a point estimate via the `guide.median()` method, which returns a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    median = {k: v.detach() for k, v in guide.median().items()}\n",
    "print(\", \".join(median.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting results\n",
    "\n",
    "Since we're fitting relative growth rate and prevalence, we can characterize all lineages by these quantities. Note the relative transmissibility looks suspciously diverse, suggesting we should probably add process noise to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_prevalence(place_query=\"\", max_len=999):\n",
    "    ids = [i for name, i in location_id.items() if place_query in name]\n",
    "    local_lineages = dense_data.sum(0)[ids].sum(0).nonzero(as_tuple=True)[0]\n",
    "    assert ids, \"no locations matching \" + place_query\n",
    "    log_rate = median[\"log_rate\"] / 12\n",
    "    log_init = median[\"log_init\"][ids].logsumexp(0)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(log_rate[local_lineages], log_init[local_lineages], s=10, color=\"#ff7777\")\n",
    "    X, Y = [], []\n",
    "    for u, v in edges.tolist():\n",
    "        if u in local_lineages and v in local_lineages:\n",
    "            X.extend([log_rate[u], log_rate[v], None])\n",
    "            Y.extend([log_init[u], log_init[v], None])\n",
    "    plt.plot(X, Y, color=\"#ff7777\", lw=0.5)\n",
    "    for name, i in lineage_id.items():\n",
    "        if i in local_lineages and len(name) <= max_len:\n",
    "            plt.text(log_rate[i], log_init[i] + 0.08, name,\n",
    "                     fontsize=5, horizontalalignment=\"center\")\n",
    "    plt.ylabel(\"log prevalence\")\n",
    "    plt.xlabel(\"relative transmissibility (log monthly growth rate)\")\n",
    "    plt.title(f\"Prevalence and transmissibility of {len(local_lineages)} lineages\"\n",
    "              + (\" in \" + place_query if place_query else \" globally\"));\n",
    "    \n",
    "plot_prevalence(max_len=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prevalence(\"Massachusetts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use our logistic curves to forecast lineage prevalence in each region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decomposition(queries, num_parts=7):\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "    fig, axes = plt.subplots(len(queries), figsize=(8, 1 + 1.2 * len(queries)), sharex=True)\n",
    "    for row, (place_query, ax) in enumerate(zip(queries, axes)):\n",
    "        names = [name for name, i in location_id.items() if place_query in name]\n",
    "        ids = [location_id[name] for name in names]\n",
    "        assert ids, \"no locations matching \" + place_query\n",
    "        log_rate = median[\"log_rate\"]\n",
    "        # FIXME this ignores region population when aggregating:\n",
    "        log_init = median[\"log_init\"][ids].logsumexp(0)\n",
    "        assert log_init.shape == log_rate.shape\n",
    "        time = torch.linspace(0, 0.5, 100)\n",
    "        portion = (log_init + log_rate * time[:, None]).softmax(-1)\n",
    "\n",
    "        # Aggregate into top + others.\n",
    "        best = portion.sum(0).sort(0, descending=True).indices\n",
    "        parts = {\"other\": None}\n",
    "        for i in range(num_parts - 1):\n",
    "            i = best[num_parts - i - 2].item()\n",
    "            parts[lineage_id_inv[i]] = portion[:, i].clone()\n",
    "            portion[:, i] = 0\n",
    "        parts[\"other\"] = portion.sum(-1)\n",
    "        months = time * 12\n",
    "\n",
    "        ax.stackplot(months, *parts.values(), labels=tuple(parts))\n",
    "        ax.set_xlim(months.min(), months.max())\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_yticks(())\n",
    "        ax.set_ylabel(names[0].split(\"/\")[-1].strip() if len(names) == 1 else place_query)\n",
    "        if row == len(axes) - 1:\n",
    "            ax.set_xlabel(\"Lineage prevalence forecast (months in future)\")\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles[::-1], labels[::-1], loc=\"lower right\", prop={\"size\": 6.5})\n",
    "    plt.subplots_adjust(hspace=0.02);\n",
    "\n",
    "plot_decomposition([\"Mass\", \"Calif\", \"Texas\", \"Florida\", \"New York\", \"USA\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
