{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini version of mutrans.py model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro.distributions as dist\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import ClippedAdam\n",
    "from pyrocov import mutrans, pangolin, stats\n",
    "\n",
    "logging.basicConfig(format=\"%(message)s\", level=logging.INFO)\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 200\n",
    "matplotlib.rcParams[\"axes.edgecolor\"] = \"gray\"\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial', 'Avenir', 'DejaVu Sans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_subset(*args, **kwargs):\n",
    "    filename = \"results/mutrans.data.single.pt\"\n",
    "    if os.path.exists(filename):\n",
    "        dataset = torch.load(filename)\n",
    "    else:\n",
    "        dataset = mutrans.load_gisaid_data()\n",
    "        torch.save(dataset, filename)\n",
    "    dataset = mutrans.subset_gisaid_data(dataset, *args, **kwargs)\n",
    "    dataset.update(mutrans.load_jhu_data(dataset))\n",
    "    return dataset\n",
    "\n",
    "dataset = load_data_subset(\n",
    "    [\"United Kingdom\"],\n",
    "    max_strains=4,\n",
    "    obs_scale=1.0,\n",
    "    obs_max=10,\n",
    "    round_method=None,\n",
    ")\n",
    "locals().update(dataset)\n",
    "print(\", \".join(dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit = mutrans.fit_svi(\n",
    "    dataset,\n",
    "    model_type=\"\",\n",
    "    guide_type=\"map\",\n",
    "    learning_rate=0.02,\n",
    "    learning_rate_decay=1,\n",
    "    num_steps=1001,\n",
    "    num_particles=1,\n",
    "    clip_norm=10.0,\n",
    "    log_every=100,\n",
    "    seed=20210319,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit():\n",
    "    num_nonzero = int(torch.count_nonzero(weekly_strains))\n",
    "    median = fit[\"median\"]\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    time = np.arange(1, 1 + len(fit[\"losses\"]))\n",
    "    plt.plot(fit[\"losses\"], \"k--\", label=\"loss\")\n",
    "    locs = []\n",
    "    grads = []\n",
    "    for name, series in fit[\"series\"].items():\n",
    "        rankby = -torch.tensor(series).log1p().mean().item()\n",
    "        if name.startswith(\"Guide.\"):\n",
    "            name = name[len(\"Guide.\"):].replace(\"$$$\", \".\")\n",
    "            grads.append((name, series, rankby))\n",
    "        elif name != \"loss\":\n",
    "            locs.append((name, series, rankby))\n",
    "    locs.sort(key=lambda x: x[-1])\n",
    "    grads.sort(key=lambda x: x[-1])\n",
    "    for name, series, _ in locs:\n",
    "        plt.plot(time, series, label=name)\n",
    "    for name, series, _ in locs:\n",
    "        plt.plot(time, series, color=\"white\", lw=3, alpha=0.3, zorder=-1)\n",
    "    for name, series, _ in grads:\n",
    "        plt.plot(time, series, lw=1, alpha=0.5, label=name, zorder=-2)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlim(1, len(fit[\"losses\"]))\n",
    "    plt.legend(loc=\"upper left\", fontsize=8)\n",
    "    plt.xlabel(\"SVI step (duration = {:0.1f} minutes)\".format(fit[\"walltime\"]/60))\n",
    "    plt.title(\"L={:0.4g} C={:0.3g} M={:0.3g} F={:0.3g} P={:0.3g}\"\n",
    "    .format(\n",
    "        np.median(fit[\"losses\"][-201:]) / num_nonzero,\n",
    "        float(median[\"concentration\"]),\n",
    "        float(median.get(\"mislabel\", 0)),\n",
    "        float(median[\"feature_scale\"]),\n",
    "        float(median.get(\"place_scale\", 0)),\n",
    "    ))\n",
    "plot_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_forecast(queries=None, num_strains=10):\n",
    "    if queries is None:\n",
    "        queries = list(location_id)\n",
    "    elif isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "    fig, axes = plt.subplots(len(queries), figsize=(8, 1 + 1.5 * len(queries)), sharex=True)\n",
    "    if not isinstance(axes, (list, np.ndarray)):\n",
    "        axes = [axes]\n",
    "    rate = fit[\"median\"][\"rate\"]\n",
    "    init = fit[\"median\"][\"init\"]\n",
    "    local_time = dataset[\"local_time\"]\n",
    "    probs = (init + rate * local_time[:, :, None]).softmax(-1)  # [T, P, S]\n",
    "    predicted = probs * weekly_cases[:, :, None]  # [T, P, S]\n",
    "    ids = torch.tensor([i for name, i in location_id.items()\n",
    "                        if any(q in name for q in queries)])\n",
    "    strain_ids = weekly_strains[:, ids].sum([0, 1]).sort(-1, descending=True).indices\n",
    "    strain_ids = strain_ids[:num_strains]\n",
    "    colors = [f\"C{i}\" for i in range(10)] + [\"black\"] * 90\n",
    "    assert len(colors) >= num_strains\n",
    "    light = \"#bbbbbb\"\n",
    "    for row, (query, ax) in enumerate(zip(queries, axes)):\n",
    "        ids = torch.tensor([i for name, i in location_id.items() if query in name])\n",
    "        print(f\"{query} matched {len(ids)} regions\")\n",
    "        counts = weekly_cases[:, ids].sum(1)\n",
    "        counts /= counts.max()\n",
    "        ax.plot(counts, \"k-\", color=light, lw=0.8)\n",
    "        counts = weekly_strains[:, ids].sum([1, 2])\n",
    "        counts /= counts.max()\n",
    "        ax.plot(counts, \"k--\", color=light, lw=1)\n",
    "        pred = predicted[:, ids].sum(1).clamp_(min=1e-8)\n",
    "        pred /= pred.sum(-1, True)\n",
    "        obs = weekly_strains[:, ids].sum(1)\n",
    "        error = -dist.DirichletMultinomial(\n",
    "            concentration=40*pred, validate_args=False).log_prob(obs).mean()\n",
    "        obs.clamp_(min=1e-9)\n",
    "        obs /= obs.sum(-1, True)\n",
    "        for s, color in zip(strain_ids, colors):\n",
    "            ax.plot(pred[:, s], color=color)\n",
    "            strain = lineage_id_inv[s]\n",
    "            ax.plot(obs[:, s], color=color, lw=0, marker='o', markersize=3,\n",
    "                    label={\"Q\": \"B.1.1.7\"}.get(strain, strain) if row == 0 else None)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_yticks(())\n",
    "        ax.set_ylabel(\"{}\\nerror = {:0.1f}\".format(query.replace(\" / \", \"\\n\"), error))\n",
    "        ax.set_xlim(0, len(weekly_strains))\n",
    "        if row == len(axes) - 1:\n",
    "            ax.set_xlabel(\"Time (week after 2019-12-01)\")\n",
    "        if row == 0:\n",
    "            ax.legend(loc=\"upper left\", fontsize=6)\n",
    "        elif row == 1:\n",
    "            ax.plot([], lw=0, marker='o', markersize=3, color='gray',\n",
    "                    label=\"observed portion\")\n",
    "            ax.plot([], color='gray', label=\"predicted portion\")\n",
    "            ax.plot([], \"k-\", color=light, lw=0.8, label=\"relative #cases\")\n",
    "            ax.plot([], \"k--\", color=light, lw=1, label=\"relative #samples\")\n",
    "            ax.legend(loc=\"upper left\", fontsize=8)\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "\n",
    "plot_forecast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, figsize=(6,5), sharex=True)\n",
    "bins = [0, 1, 3, 10, 30, 100, 300, 1000, 3000, 10000, 30000, 100000, 300000, 1000000]\n",
    "axes[0].set_ylabel(\"time\\nplace\\nstrain\")\n",
    "axes[0].hist(weekly_strains.reshape(-1).numpy(), log=True, bins=bins)\n",
    "axes[1].set_ylabel(\"time\\nplace\")\n",
    "axes[1].hist(weekly_strains.sum(2).reshape(-1).numpy(), log=True, bins=bins)\n",
    "axes[2].set_ylabel(\"place\")\n",
    "axes[2].hist(weekly_strains.sum([0, 2]).reshape(-1).numpy(), log=True, bins=bins)\n",
    "axes[-1].set_xscale(\"log\")\n",
    "plt.subplots_adjust(hspace=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate on different downsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_MAX = 10\n",
    "datasets = {\n",
    "    obs_max: load_data_subset(\n",
    "        [\"United Kingdom\"],\n",
    "        max_strains=4,\n",
    "        obs_scale=1.0,\n",
    "        obs_max=obs_max,\n",
    "        round_method=None,\n",
    "    )\n",
    "    for obs_max in [OBS_MAX, math.inf]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = {\n",
    "    obs_max: mutrans.fit_svi(\n",
    "        datasets[obs_max],\n",
    "        model_type=\"\",\n",
    "        guide_type=\"map\",\n",
    "        learning_rate=0.02,\n",
    "        learning_rate_decay=1,\n",
    "        num_steps=1001,\n",
    "        num_particles=1,\n",
    "        clip_norm=10.0,\n",
    "        log_every=100,\n",
    "        seed=20210319,\n",
    "    )\n",
    "    for obs_max in [OBS_MAX, math.inf]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test obs_max\\tTrain obs_max\\tloss\")\n",
    "print(\"-\" * 50)\n",
    "for test_obs_max in [OBS_MAX, math.inf]:\n",
    "    for train_obs_max in [OBS_MAX, math.inf]:\n",
    "        pyro.clear_param_store()\n",
    "        loss = fits[train_obs_max][\"guide\"].loss(datasets[test_obs_max])\n",
    "        print(f\"{test_obs_max}\\t\\t{train_obs_max}\\t\\t{loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since\n",
    "```\n",
    "loss(test=inf,train=10) > loss(test=inf,train=inf)\n",
    "```\n",
    "it appears the poor-fit issue is with the model rather than with inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
