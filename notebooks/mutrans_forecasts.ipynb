{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for mutation growth rate paper\n",
    "\n",
    "This notebook generates plots for the [paper/](paper/) directory. This assumes you've alread run\n",
    "```sh\n",
    "make update                       # Downloads data (~1hour).\n",
    "make preprocess                   # Preprocesses data (~3days on a big machine).\n",
    "python mutrans.py --vary-holdout  # Fits and crossvalidates model (~1hour GPU).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import logging\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyro.distributions as dist\n",
    "from pyro.ops.tensor_utils import convolve\n",
    "from pyrocov import mutrans, pangolin, stats\n",
    "from pyrocov.stats import normal_log10bf\n",
    "from pyrocov.util import (\n",
    "    pretty_print, pearson_correlation, quotient_central_moments, generate_colors\n",
    ")\n",
    "from pyrocov.sarscov2 import GENE_TO_POSITION, GENE_STRUCTURE, aa_mutation_to_position\n",
    "\n",
    "logging.basicConfig(format=\"%(relativeCreated) 9d %(message)s\", level=logging.INFO)\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 200\n",
    "matplotlib.rcParams[\"axes.edgecolor\"] = \"gray\"\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "matplotlib.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "matplotlib.rcParams[\"savefig.pad_inches\"] = 0.01\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial', 'Avenir', 'DejaVu Sans']\n",
    "matplotlib.rcParams.update({\n",
    "    # 'text.usetex': True,\n",
    "    'text.latex.preamble': r'\\usepackage{amsfonts}',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_clades = 3000\n",
    "min_num_mutations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def load_data():\n",
    "    filename = f\"results/mutrans.data.single.{max_num_clades}.{min_num_mutations}.50.None.pt\"\n",
    "    dataset = torch.load(filename, map_location=\"cpu\")\n",
    "    dataset.update(mutrans.load_jhu_data(dataset))\n",
    "    return dataset\n",
    "dataset = load_data()\n",
    "locals().update(dataset)\n",
    "for k, v in sorted(dataset.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k} \\t{type(v).__name__} of shape {tuple(v.shape)}\")\n",
    "    else:\n",
    "        print(f\"{k} \\t{type(v).__name__} of size {len(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dense mapping between fine clades and Pango lineages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} x {} x {} = {}\".format(*weekly_clades.shape, weekly_clades.shape.numel()))\n",
    "print(int(weekly_clades.sum()))\n",
    "print(weekly_clades.ne(0).float().mean().item())\n",
    "print(weekly_clades.ne(0).any(0).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/columns.{max_num_clades}.pkl\", \"rb\") as f:\n",
    "    columns = pickle.load(f)\n",
    "print(\"Loaded data from {} samples\".format(len(columns[\"lineage\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"results/nextclade.counts.pkl\", \"rb\") as f:\n",
    "        all_mutations = pickle.load(f)\n",
    "except Exception:\n",
    "    with open(\"results/stats.pkl\", \"rb\") as f:\n",
    "        all_mutations = pickle.load(f)[\"aaSubstitutions\"]\n",
    "print(f\"Loaded {len(all_mutations)} mutations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checking case count time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(weekly_cases, lw=1, alpha=0.5)\n",
    "    plt.yscale(\"symlog\", linthresh=10)\n",
    "    plt.ylim(0, None)\n",
    "    plt.xlim(0, len(weekly_cases) - 1)\n",
    "    plt.xlabel(\"week after 2019-12-01\")\n",
    "    plt.ylabel(\"confirmed cases\");\n",
    "\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.plot(weekly_clades.sum(-1), lw=1, alpha=0.5)\n",
    "    plt.yscale(\"symlog\", linthresh=10)\n",
    "    plt.ylim(0, None)\n",
    "    plt.xlim(0, len(weekly_cases) - 1)\n",
    "    plt.xlabel(\"week after 2019-12-01\")\n",
    "    plt.ylabel(\"sequenced samples\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = set(location_id)\n",
    "N_usa = sum(1 for k in locations if \"/ USA /\" in k)\n",
    "N_uk = sum(1 for k in locations if \"/ United Kingdom /\" in k)\n",
    "N_other = len(locations) - N_usa - N_uk\n",
    "print(N_usa, N_uk, N_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll account for epidemiological dynamics in the form of random drift on top of our logistic growth model. Since random drift is inversely proportional to the local number of infections, we'll need a new data source for the number of infections in each region. We'll use JHU's confirmed case counts time series as a proxy for the number of total infections in each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = torch.load(\"results/mutrans.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in fits:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit = list(fits.values())[0]\n",
    "pretty_print(best_fit, max_items=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale `coef` by 1/100 in all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALREADY_SCALED = set()\n",
    "\n",
    "def scale_tensors(x, names={\"coef\"}, scale=0.01, prefix=\"\"):\n",
    "    if id(x) in ALREADY_SCALED:\n",
    "        return\n",
    "    if isinstance(x, dict):\n",
    "        for k, v in list(x.items()):\n",
    "            if k in names:\n",
    "                print(f\"{prefix}.{k}\")\n",
    "                x[k] = v * scale\n",
    "            elif k == \"diagnostics\":\n",
    "                continue\n",
    "            else:\n",
    "                scale_tensors(v, names, scale, f\"{prefix}.{k}\")\n",
    "    ALREADY_SCALED.add(id(x))\n",
    "                \n",
    "scale_tensors(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(best_fit[\"params\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    plt.plot(\n",
    "        best_fit[\"mean\"][\"init_loc\"] + 0 * best_fit[\"median\"][\"init\"],\n",
    "        best_fit[\"median\"][\"init\"],\n",
    "        \"k.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Assess model fitness"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_fit(key, fit, filenames=()):\n",
    "    num_nonzero = int(torch.count_nonzero(weekly_clades))\n",
    "    median = fit.get(\"median\", fit.get(\"mean\", {}))\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    time = np.arange(1, 1 + len(fit[\"losses\"]))\n",
    "    plt.plot(fit[\"losses\"], \"k--\", label=\"ELBO loss\" if filenames else \"loss\")\n",
    "    locs = []\n",
    "    grads = []\n",
    "    for name, series in fit[\"series\"].items():\n",
    "        rankby = -torch.tensor(series).log1p().mean().item()\n",
    "        if name.startswith(\"Guide.\") or name.startswith(\"Auto\"):\n",
    "            name = name[len(\"Guide.\"):].replace(\"$$$\", \".\")\n",
    "            grads.append((name, series, rankby))\n",
    "        elif name.endswith(\"_centered\") or name == \"time_shift\":\n",
    "            grads.append((name, series, rankby))\n",
    "        elif name != \"loss\":\n",
    "            locs.append((name, series, rankby))\n",
    "    locs.sort(key=lambda x: x[-1])\n",
    "    grads.sort(key=lambda x: x[-1])\n",
    "    aliases = {\n",
    "        \"init_loc_scale\": r\"E[$\\sigma_1$]\",\n",
    "        \"init_scale\": r\"E[$\\sigma_2$]\",\n",
    "        \"coef_scale\": r\"E[$\\sigma_3$]\",\n",
    "        \"rate_scale\": r\"E[$\\sigma_4$]\",\n",
    "    }\n",
    "    for name, series, _ in locs:\n",
    "        if filenames:\n",
    "            if name not in aliases:\n",
    "                continue\n",
    "            name = aliases.get(name, name)\n",
    "        plt.plot(time, series, label=name)\n",
    "    for name, series, _ in locs:\n",
    "        plt.plot(time, series, color=\"white\", lw=3, alpha=0.3, zorder=-1)\n",
    "    if not filenames:\n",
    "        for name, series, _ in grads:\n",
    "            if filenames:\n",
    "                name = None\n",
    "            plt.plot(time, series, lw=1, alpha=0.3, label=name, zorder=-2)\n",
    "    if not filenames:\n",
    "        plt.plot([], [], \"k-\", lw=1, alpha=0.3, label=\"gradient norm\", zorder=-2)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlim(1, len(fit[\"losses\"]))\n",
    "    plt.legend(loc=\"upper left\", fontsize=6)\n",
    "    plt.xlabel(\"SVI step (duration = {:0.1f} minutes)\".format(fit[\"walltime\"]/60))\n",
    "    loss = np.median(fit[\"losses\"][-201:]) / num_nonzero\n",
    "    scalars = \" \".join([f\"L={loss:0.6g}\"] + [\n",
    "        \"{}={:0.3g}\".format(\n",
    "            \"\".join(p[0] for p in k.split(\"_\")).upper(), v\n",
    "        )\n",
    "        for k, v in median.items()\n",
    "        if v.numel() == 1\n",
    "    ])\n",
    "    plt.title(\"{} ({})\\n{}\".format(key[0], scalars, key[-1]))\n",
    "    for filename in filenames:\n",
    "        plt.savefig(filename)\n",
    "\n",
    "plot_fit(*list(fits.items())[0], filenames=[\"paper/convergence.png\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_fits():\n",
    "    for key, fit in fits.items():\n",
    "        plot_fit(key, fit)\n",
    "plot_fits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plusminus(mean, std):\n",
    "    p95 = 1.96 * std\n",
    "    return torch.stack([mean - p95, mean, mean + p95])\n",
    "\n",
    "def plot_forecast(fit, queries=None, num_strains=10, filenames=[]):\n",
    "    if queries is None:\n",
    "        queries = list(location_id)\n",
    "    elif isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "    fig, axes = plt.subplots(len(queries), figsize=(8, 0.5 + 2.5 * len(queries)), sharex=True)\n",
    "    if not isinstance(axes, (list, np.ndarray)):\n",
    "        axes = [axes]\n",
    "    dates = matplotlib.dates.date2num(mutrans.date_range(len(fit[\"mean\"][\"probs\"])))\n",
    "    forecast_steps = len(fit[\"mean\"][\"probs\"]) - len(weekly_cases)\n",
    "    assert forecast_steps >= 0\n",
    "    probs = plusminus(fit[\"mean\"][\"probs\"], fit[\"std\"][\"probs\"])  # [3, T, P, L]\n",
    "    padding = 1 + weekly_cases.mean(0, True).expand(forecast_steps, -1)\n",
    "    weekly_cases_ = torch.cat([weekly_cases, padding], 0)\n",
    "    weekly_cases_.add_(1)  # avoid divide by zero\n",
    "    predicted = probs * weekly_cases_[..., None]\n",
    "    L = probs.shape[-1]\n",
    "    weekly_lineages = weekly_clades.new_zeros(weekly_clades.shape[:-1] + (L,)).scatter_add_(\n",
    "        -1, clade_id_to_lineage_id.expand_as(weekly_clades), weekly_clades\n",
    "    )\n",
    "    ids = torch.tensor([i for i, name in enumerate(location_id_inv)\n",
    "                        if any(q in name for q in queries)])\n",
    "    \n",
    "    T = weekly_lineages.shape[0]\n",
    "    early_strain_ids = weekly_lineages[:(T-8), ids].sum([0, 1]).sort(-1, descending=True).indices\n",
    "    late_strain_ids = weekly_lineages[(T-8):, ids].sum([0, 1]).sort(-1, descending=True).indices\n",
    "    strain_ids = torch.cat((early_strain_ids[:(num_strains//2)], late_strain_ids[:(num_strains - num_strains//2)]))\n",
    "    print(type(strain_ids))\n",
    "    \n",
    "    #strain_ids = weekly_lineages[:, ids].sum([0, 1]).sort(-1, descending=True).indices\n",
    "    #strain_ids = strain_ids[:num_strains]\n",
    "    colors = generate_colors()\n",
    "    assert len(colors) >= num_strains\n",
    "    light = \"#bbbbbb\"\n",
    "    for row, (query, ax) in enumerate(zip(queries, axes)):\n",
    "        ids = torch.tensor([i for i, name in enumerate(location_id_inv) if query in name])\n",
    "        print(f\"{query} matched {len(ids)} regions\")\n",
    "        if len(axes) > 1:\n",
    "            counts = weekly_cases[:, ids].sum(1)\n",
    "            print(f\"{query}: max {counts.max():g}, total {counts.sum():g}\")\n",
    "            counts /= counts.max()\n",
    "            ax.plot(dates[:len(counts)], counts, linestyle=\"-\", color=light, lw=0.8, zorder=-20)\n",
    "            counts = weekly_lineages[:, ids].sum([1, 2])\n",
    "            counts /= counts.max()\n",
    "            ax.plot(dates[:len(counts)], counts, linestyle=\"--\", color=light, lw=1, zorder=-20)\n",
    "        pred = predicted.index_select(-2, ids).sum(-2)\n",
    "        pred /= pred[1].sum(-1, True).clamp_(min=1e-20)\n",
    "        obs = weekly_lineages[:, ids].sum(1)\n",
    "        obs /= obs.sum(-1, True).clamp_(min=1e-9)\n",
    "        for s, color in zip(strain_ids, colors):\n",
    "            lb, mean, ub = pred[..., s]\n",
    "            ax.fill_between(dates, lb, ub, color=color, alpha=0.2, zorder=-10)\n",
    "            ax.plot(dates, mean, color=color, lw=1, zorder=-9)\n",
    "            lineage = lineage_id_inv[s]\n",
    "            ax.plot(dates[:len(obs)], obs[:, s], color=color, lw=0, marker='o', markersize=3,\n",
    "                    label=lineage if row == 0 else None)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_yticks(())\n",
    "        ax.set_ylabel(query.replace(\" / \", \"\\n\"))\n",
    "        ax.set_xlim(dates.min(), dates.max())\n",
    "        if row == 0:\n",
    "            ax.legend(loc=\"upper left\", fontsize=8 * (10 / num_strains) ** 0.8)\n",
    "        elif row == 1:\n",
    "            ax.plot([], linestyle=\"--\", color=light, lw=1, label=\"relative #samples\")\n",
    "            ax.plot([], linestyle=\"-\", color=light, lw=0.8, label=\"relative #cases\")\n",
    "            ax.plot([], lw=0, marker='o', markersize=3, color='gray',\n",
    "                    label=\"observed portion\")\n",
    "            ax.fill_between([], [], [], color='gray', label=\"predicted portion\")\n",
    "            ax.legend(loc=\"upper left\",)\n",
    "    ax.xaxis.set_major_locator(matplotlib.dates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%b %Y\"))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    for filename in filenames:\n",
    "        plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in location_id.keys():\n",
    "    name = loc.replace(' / ','_')\n",
    "    print(name)\n",
    "    plot_forecast(best_fit,\n",
    "        queries=[loc],\n",
    "        num_strains=15,\n",
    "        filenames=[f\"paper/per_region_forecasts/{name}.png\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
