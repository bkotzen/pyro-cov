{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting Plots for mutation growth rate paper\n",
    "\n",
    "This notebook generates plots for the [paper/backtesting](paper/backtesting) directory. This assumes you've alread run\n",
    "```sh\n",
    "make update                       # Downloads data (~1hour).\n",
    "make preprocess-usher             # Preprocesses usher tree\n",
    "make backtesting-complete                  # Fits backtesting models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import logging\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyro.distributions as dist\n",
    "from pyrocov import mutrans, pangolin, stats\n",
    "from pyrocov.stats import normal_log10bf\n",
    "from pyrocov.util import pretty_print, pearson_correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import colorcet as cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"figure.dpi\"] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging\n",
    "logging.basicConfig(format=\"%(relativeCreated) 9d %(message)s\", level=logging.INFO)\n",
    "# This line can be used to modify logging as required later in the notebook\n",
    "#logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib params\n",
    "#matplotlib.rcParams[\"figure.dpi\"] = 200\n",
    "#matplotlib.rcParams['figure.figsize'] = [8, 8]\n",
    "matplotlib.rcParams[\"axes.edgecolor\"] = \"gray\"\n",
    "matplotlib.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial', 'Avenir', 'DejaVu Sans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire constant dataset\n",
    "max_num_clades = 3000\n",
    "min_num_mutations = 1\n",
    "min_region_size = 50\n",
    "ambiguous = False\n",
    "columns_filename=f\"results/columns.{max_num_clades}.pkl\"\n",
    "features_filename=f\"results/features.{max_num_clades}.{min_num_mutations}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = mutrans.load_gisaid_data(\n",
    "        device=\"cpu\",\n",
    "        columns_filename=columns_filename,\n",
    "        features_filename=features_filename,\n",
    "        min_region_size=min_region_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load backtesting trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = torch.load(\"results/mutrans.backtesting.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We have loaded {len(fits)} models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print info on available models and what the keys are\n",
    "if True:\n",
    "    for key in fits:\n",
    "        print(f'{key} -- {fits[key][\"weekly_clades_shape\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale `coef` by 1/100 in all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALREADY_SCALED = set()\n",
    "\n",
    "def scale_tensors(x, names={\"coef\"}, scale=0.01, prefix=\"\", verbose=True):\n",
    "    if id(x) in ALREADY_SCALED:\n",
    "        return\n",
    "    if isinstance(x, dict):\n",
    "        for k, v in list(x.items()):\n",
    "            if k in names:\n",
    "                if verbose:\n",
    "                    print(f\"{prefix}.{k}\")\n",
    "                x[k] = v * scale\n",
    "            elif k == \"diagnostics\":\n",
    "                continue\n",
    "            else:\n",
    "                scale_tensors(v, names, scale, f\"{prefix}.{k}\", verbose=verbose)\n",
    "    ALREADY_SCALED.add(id(x))\n",
    "                \n",
    "scale_tensors(fits, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_dir_prefix = \"paper/backtesting/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_clades_to_lineages(weekly_clades, clade_id_to_lineage_id, n_model_lineages):\n",
    "    weekly_lineages = weekly_clades.new_zeros(weekly_clades.shape[:-1] + (n_model_lineages,)).scatter_add_(\n",
    "        -1, clade_id_to_lineage_id.expand_as(weekly_clades), weekly_clades)\n",
    "    return weekly_lineages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plusminus(mean, std):\n",
    "    p95 = 1.96 * std\n",
    "    return torch.stack([mean - p95, mean, mean + p95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrocov.util import (\n",
    "    pretty_print, pearson_correlation, quotient_central_moments, generate_colors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_lineages_for_plot(\n",
    "        weekly_lineages,\n",
    "        num_lineages,\n",
    "        lineage_id_inv,\n",
    "        location_ids, # location ids\n",
    "        nbins = 10,\n",
    "        additional_lineages = [],\n",
    "    ):\n",
    "    \"\"\"Return names of lineages for plot\"\"\"\n",
    "    \n",
    "    keep_per_bin = math.ceil(num_lineages / nbins)\n",
    "    T = weekly_lineages.shape[0]\n",
    "    time_intervals = list(split(np.arange(T), nbins))\n",
    "    lineage_ids = []\n",
    "    for interval in time_intervals:\n",
    "        kept_lineage_ids = weekly_lineages[interval][:, location_ids].sum([0, 1]).sort(-1, descending=True).indices[:keep_per_bin]\n",
    "        lineage_ids.append(kept_lineage_ids)\n",
    "    lineage_ids = torch.cat(lineage_ids)\n",
    "    additional_indexes = list(lineage_id_inv.index(x) for x in additional_lineages)\n",
    "    lineage_ids = torch.cat((lineage_ids, torch.tensor(additional_indexes))).tolist()\n",
    "\n",
    "    return sorted(set(lineage_id_inv[x] for x in lineage_ids)) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generate_colors_from_lineage_names(lineage_names):\n",
    "    \n",
    "    standard_colors_dict = {\n",
    "        'BA.1': cc.glasbey[0],\n",
    "        'BA.2': cc.glasbey[1],\n",
    "        'BA.1.1': cc.glasbey[2],\n",
    "        'AY.4': cc.glasbey[3],\n",
    "        'B.1.1.7': cc.glasbey[4],\n",
    "        'B.1.1': cc.glasbey[5],\n",
    "        'B.1.177': cc.glasbey[6],\n",
    "    }\n",
    "    \n",
    "    glasbey_offset = len(standard_colors_dict)\n",
    "    \n",
    "    colors = []\n",
    "    for lineage_name in lineage_names:\n",
    "        try:\n",
    "            color = standard_colors_dict[lineage_name]\n",
    "        except KeyError:\n",
    "            color = cc.glasbey[glasbey_offset]\n",
    "            glasbey_offset += 1\n",
    "        colors.append(color)\n",
    "        \n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits[list(fits.keys())[0]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rainbow(fit, lineage_names, input_dataset):\n",
    "    clade_id = input_dataset['clade_id']\n",
    "    lineage_to_clade = input_dataset['lineage_to_clade']\n",
    "\n",
    "    rate = fit[\"mean\"][\"rate\"].mean(0) # Mean over places\n",
    "    rates = torch.stack([\n",
    "        rate[clade_id[lineage_to_clade[l]]] for l in lineage_names\n",
    "    ])\n",
    "    C = len(lineage_names)\n",
    "    colors = [None] * C\n",
    "    for c, l in enumerate(rates.sort(0).indices.tolist()):\n",
    "        colors[l] = cm.rainbow(c / (C - 1))\n",
    "    return {\n",
    "        'colors': colors,\n",
    "        'min': rates.min().item(),\n",
    "        'max': rates.max().item(),\n",
    "        'rates': rates,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_forecast2(fit, input_dataset, queries, num_lineages=10, filenames=[], \n",
    "                   verbose=False, additional_lineages = ['BA.2'], nbins=5, \n",
    "                   legend_out=False, figsize_x = None, figsize_y = None, \n",
    "                   auto_select_lineages = True, colors_dict_export = None,\n",
    "                  show_legend = True, show_case_counts = True, show_second_legend = True):\n",
    "    # Convert queries to array if only only string\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "    \n",
    "    # Get dimensions of the model fit (T,P,L) these are probabilities\n",
    "    n_model_periods, n_model_places, n_model_lineages = fit['mean']['probs'].shape\n",
    "    if (verbose):\n",
    "        print('---')\n",
    "        print(f'n_model_periods: {n_model_periods}')\n",
    "        print(f'n_model_places: {n_model_places}')\n",
    "        print(f'n_model_lineages: {n_model_lineages}')\n",
    "    \n",
    "    # Get dimensions of weekly_cases (T,P) these are JHU counts\n",
    "    weekly_cases_fit = fit['weekly_cases']\n",
    "    n_cases_periods, n_cases_places = weekly_cases_fit.shape\n",
    "    if (verbose):\n",
    "        print('---')\n",
    "        print(f'n_cases_periods: {n_cases_periods}')\n",
    "        print(f'n_cases_places: {n_cases_places}')\n",
    "    \n",
    "    # Some checks\n",
    "    assert n_cases_places == n_model_places\n",
    "    assert n_model_periods > n_cases_periods\n",
    "    \n",
    "    # Calculate how many periods are forecasted (i.e. are beyond the input to the model)\n",
    "    n_forecast_steps = n_model_periods - n_cases_periods\n",
    "    if (verbose):\n",
    "        print(f'n_forecast_steps: {n_forecast_steps}')\n",
    "        \n",
    "    # Weekly case counts by time place and clade obtained from the fit\n",
    "    weekly_clades_fit = fit['weekly_clades'] # T, P, C\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'weekly_clades_fit shape: {weekly_clades_fit.shape}')\n",
    "    \n",
    "    # Weekly case counts by time place and clade obtain from the input data\n",
    "    # This has more time point and more regions than the one from the fit\n",
    "    weekly_clades_data = input_dataset['weekly_clades']\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'weekly_clades_data shape: {weekly_clades_data.shape}')\n",
    "    \n",
    "    # Mapping from clades to lineages, a tensor of indexes\n",
    "    # This is valid for both the fit and the input_data\n",
    "    clade_id_to_lineage_id = input_dataset['clade_id_to_lineage_id']\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'clade_id_to_lineage_id length: {len(clade_id_to_lineage_id)}')\n",
    "        \n",
    "    # We don't have clade_id_to_lineage_id in the fit -- it should in principle be the same\n",
    "    \n",
    "    # Summarize the counts of the weekly_clades (from data or fit) to the number of lineages in the model\n",
    "    weekly_lineages_data = weekly_clades_to_lineages(weekly_clades_data, clade_id_to_lineage_id, n_model_lineages)\n",
    "    weekly_lineages_fit = weekly_clades_to_lineages(weekly_clades_fit, clade_id_to_lineage_id, n_model_lineages)\n",
    "    \n",
    "    # Add CI to the probs\n",
    "    probs = plusminus(fit['mean']['probs'], fit['std']['probs']) # [3,T,P,L]\n",
    "    \n",
    "    # Expand weekly_cases_fit (JHU counts) from the model to cover the steps we are forecasting\n",
    "    padding = 1 + weekly_cases_fit.mean(0, keepdim=True).expand(n_forecast_steps, -1)\n",
    "    weekly_cases_fit_ = torch.cat([weekly_cases_fit, padding], 0)\n",
    "    weekly_cases_fit_.add_(10)\n",
    "    # Generate predictions\n",
    "    # Note: For the evaluation maybe we are better off comparing probabilities not counts\n",
    "    predicted = probs * weekly_cases_fit_[..., None]\n",
    "    \n",
    "    # This is an array of strings listing the locations for the data\n",
    "    location_id_inv_data = input_dataset['location_id_inv']\n",
    "    if (verbose):\n",
    "        print('---')\n",
    "        print(f'location_id_inv_data length: {len(location_id_inv_data)}')\n",
    "    \n",
    "    # This is an array of strings listing the locations for the fit\n",
    "    location_id_inv_fit = fit['location_id_inv']\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'location_id_inv_fit length: {len(location_id_inv_fit)}')\n",
    "    \n",
    "    # Get the location indexes that we want to keep based on query for the data\n",
    "    ids_fit = torch.tensor([i for i, name in enumerate(location_id_inv_fit) if any(q in name for q in queries)])\n",
    "    \n",
    "    # These are the lineage labels, we can get them from either the fit or the dataset. \n",
    "    # We assume that these are identical and we assert this below\n",
    "    lineage_id_inv_fit = fit['lineage_id_inv']\n",
    "    lineage_id_inv_data = input_dataset['lineage_id_inv']\n",
    "    assert lineage_id_inv_fit == lineage_id_inv_data\n",
    "    \n",
    "    # Subset weekly_lineages_fit to those location sum over time and place and get the indices in descending order\n",
    "    plot_lineages_ids_inv_fit = select_lineages_for_plot(\n",
    "        weekly_lineages = weekly_lineages_fit,\n",
    "        num_lineages = num_lineages,\n",
    "        lineage_id_inv = lineage_id_inv_fit,\n",
    "        location_ids = ids_fit, \n",
    "        nbins = nbins,\n",
    "        additional_lineages = additional_lineages,\n",
    "    )\n",
    "    \n",
    "    # tbw\n",
    "    plot_lineages_ids_inv_pred = select_lineages_for_plot(\n",
    "        weekly_lineages = fit['mean']['probs'],\n",
    "        num_lineages = num_lineages,\n",
    "        lineage_id_inv = lineage_id_inv_fit,\n",
    "        location_ids = ids_fit, \n",
    "        nbins = nbins,\n",
    "        additional_lineages = additional_lineages,\n",
    "    )\n",
    "\n",
    "    # Same thing for the data\n",
    "    ids_data = torch.tensor([ i for i, name in enumerate(location_id_inv_data) if any(q in name for q in queries)])\n",
    "    \n",
    "    # Subset weekly_lineages_fit to those location sum over time and place and get the indices in descending order\n",
    "    plot_lineages_ids_inv_data = select_lineages_for_plot(\n",
    "        weekly_lineages = weekly_lineages_data,\n",
    "        num_lineages = num_lineages,\n",
    "        lineage_id_inv = lineage_id_inv_data,\n",
    "        location_ids = ids_data, \n",
    "        nbins = nbins,\n",
    "        additional_lineages = additional_lineages,\n",
    "    )\n",
    "    \n",
    "    # merge the lineage name from datset and fit to get a single list\n",
    "    lineage_name_to_index_map_data = { l:i for i, l in enumerate(lineage_id_inv_data)}\n",
    "    lineage_name_to_index_map_fit = { l:i for i, l in enumerate(lineage_id_inv_fit)}\n",
    "    \n",
    "    if auto_select_lineages:\n",
    "        plot_lineages_ids_inv_joint = sorted(\n",
    "            set(plot_lineages_ids_inv_fit)\n",
    "                .union(plot_lineages_ids_inv_data)\n",
    "                .union(plot_lineages_ids_inv_pred))\n",
    "        colors_dict = generate_rainbow(fit, plot_lineages_ids_inv_joint, input_dataset)\n",
    "        colors = colors_dict['colors']\n",
    "        rates = colors_dict['rates']\n",
    "        order_perm = rates.argsort().numpy()[::-1]\n",
    "        \n",
    "        # we may have a few plotted lineages now...\n",
    "        num_lineages = len(plot_lineages_ids_inv_joint)\n",
    "\n",
    "        lineage_ids_fit = list(map(lineage_name_to_index_map_fit.get, plot_lineages_ids_inv_joint))\n",
    "        lineage_ids_data = list(map(lineage_name_to_index_map_data.get, plot_lineages_ids_inv_joint))\n",
    "        assert lineage_ids_fit == lineage_ids_data\n",
    "\n",
    "        # reorder lineage_ids_data, lineage_ids_fit, colors\n",
    "        plot_lineages_ids_inv_joint = np.asarray(plot_lineages_ids_inv_joint)[order_perm]\n",
    "        lineage_ids_data = np.asarray(lineage_ids_data)[order_perm]\n",
    "        lineage_ids_fit = np.asarray(lineage_ids_fit)[order_perm]\n",
    "        colors = np.asarray(colors)[order_perm]\n",
    "        colors_dict_export = {l:c for l, c in zip(plot_lineages_ids_inv_joint, colors)}\n",
    "        \n",
    "        rates = np.asarray(rates)[order_perm]\n",
    "    \n",
    "    else:\n",
    "        rates = None\n",
    "        assert colors_dict_export is not None\n",
    "        assert additional_lineages is not None\n",
    "        \n",
    "        plot_lineages_ids_inv_joint = additional_lineages\n",
    "        num_lineages = len(plot_lineages_ids_inv_joint)\n",
    "        \n",
    "        lineage_ids_fit = list(map(lineage_name_to_index_map_fit.get, plot_lineages_ids_inv_joint))\n",
    "        lineage_ids_data = list(map(lineage_name_to_index_map_data.get, plot_lineages_ids_inv_joint))\n",
    "        assert lineage_ids_fit == lineage_ids_data\n",
    "        \n",
    "        # grab colors from the provided dictionary\n",
    "        colors = list(map(colors_dict_export.get, plot_lineages_ids_inv_joint))\n",
    "\n",
    "\n",
    "    \n",
    "    assert len(colors) >= num_lineages\n",
    "    light = '#bbbbbb'\n",
    "    dark = '#444444'\n",
    "    \n",
    "    # Generate Figure\n",
    "    if figsize_x is None:\n",
    "        figsize_x = 8\n",
    "        \n",
    "    if figsize_y is None:\n",
    "        figsize_y = 0.5 + 2.5 * len(queries)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(queries), figsize=(figsize_x, figsize_y), sharex=True)\n",
    "    if not isinstance(axes, (list, np.ndarray)):\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Get x axis dates for plotting\n",
    "    dates = matplotlib.dates.date2num(mutrans.date_range(len(fit[\"mean\"][\"probs\"])))\n",
    "\n",
    "    # Query (region) plotting loop\n",
    "    for row, (query, ax) in enumerate(zip(queries, axes)):\n",
    "        # location ids for this query (some queries are made of multiple regions)\n",
    "        ids_fit = torch.tensor([i for i, name in enumerate(location_id_inv_fit) if query in name])\n",
    "        if verbose:\n",
    "            print('---')\n",
    "            print(f\"{query} matched {len(ids_fit)} regions in the fit\")\n",
    "        \n",
    "        # location ids for this query in the data\n",
    "        ids_data = torch.tensor([i for i, name in enumerate(location_id_inv_data) if query in name])\n",
    "        \n",
    "        if len(axes) > 1 and show_case_counts:\n",
    "            # Plot weekly cases total\n",
    "            counts = weekly_cases_fit[:, ids_fit].sum(1)\n",
    "            if verbose:\n",
    "                print(f\"{query}: max {counts.max():g}, total {counts.sum():g}\")\n",
    "            counts /= counts.max()\n",
    "            ax.plot(dates[:len(counts)], counts, \"k-\", color=light, lw=0.8, zorder=-20)\n",
    "            \n",
    "            # Plot weekly lineages total we are getting the data from the fit not the dataset\n",
    "            counts = weekly_lineages_fit[:, ids_fit].sum([1, 2])\n",
    "            counts /= counts.max()\n",
    "            ax.plot(dates[:len(counts)], counts, \"k--\", color=light, lw=1, zorder=-20)\n",
    "            \n",
    "        # Get the predictions for the relevant regions, normalize\n",
    "        pred = predicted.index_select(-2, ids_fit).sum(-2)\n",
    "        pred /= pred[1].sum(-1, True).clamp_(min=1e-20)\n",
    "        \n",
    "        # Get the observations for the relevant regions\n",
    "        obs = weekly_lineages_fit[:, ids_fit].sum(1)\n",
    "        obs /= obs.sum(-1, True).clamp_(min=1e-9)\n",
    "        \n",
    "        # Observations from the data -- this extends further in the time dimension\n",
    "        obs_data = weekly_lineages_data[:, ids_data].sum(1)\n",
    "        obs_data /= obs_data.sum(-1, True).clamp(min=1e-9)\n",
    "        \n",
    "        # lineage plotting loop\n",
    "        for s, color in zip(lineage_ids_fit, colors):\n",
    "            lb, mean, ub = pred[..., s]\n",
    "            ax.fill_between(dates, lb, ub, color=color, alpha=0.2, zorder=-10)\n",
    "            ax.plot(dates, mean, color=color, lw=1, zorder=-9)\n",
    "            # Get the lineage label\n",
    "            lineage = lineage_id_inv_fit[s]\n",
    "            ax.plot(dates[:len(obs)], obs[:, s], color=color, lw=0, marker='o', markersize=3,\n",
    "                    label=lineage if row == 0 else None)\n",
    "        \n",
    "        # Plot observations from the dataset for all the forecast points\n",
    "        # TODO: Fix colors to match (we probably want to fix \"sort(-1, descending=True)\" to be a matching permutation instead)\n",
    "        for s, color in zip(lineage_ids_data, colors):\n",
    "            lineage = lineage_id_inv_data[s]\n",
    "            max_time_step = min((len(obs)+n_forecast_steps), obs_data.shape[0]-1)\n",
    "            \n",
    "            ax.plot(dates[len(obs):max_time_step], obs_data[len(obs):max_time_step, s], label='_nolegend_',\n",
    "                    color=color, lw=0, marker='x', markersize=2)\n",
    "            \n",
    "        # Add shading for the forecast region\n",
    "        ax.axvline(dates[len(obs)], linestyle='--', lw=1, color=(0.5, 0.5, 0.5))\n",
    "        ax.axvspan(dates[len(obs)],dates[len(obs)+n_forecast_steps-1], color=(0.5, 0.5, 0.5), alpha=0.2)\n",
    "        \n",
    "        # Set axis ticks\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_yticks(())\n",
    "        ax.set_ylabel(query.replace(\" / \", \"\\n\"))\n",
    "        ax.set_xlim(dates.min(), dates.max())\n",
    "        \n",
    "        # Print legend\n",
    "        if show_legend:\n",
    "            if legend_out:\n",
    "\n",
    "                if row == 0:\n",
    "                    ax.legend(loc=\"upper left\", bbox_to_anchor=(1.01, 1.04), fontsize=10)\n",
    "                elif row == 1:\n",
    "                    if show_case_counts and show_second_legend:\n",
    "                        ax.plot([], \"k--\", color=light, lw=1, label=\"relative #samples\")\n",
    "                        ax.plot([], \"k-\", color=light, lw=0.8, label=\"relative #cases\")\n",
    "                        ax.plot([], lw=0, marker='o', markersize=3, color='gray',\n",
    "                                label=\"observed portion\")\n",
    "                        ax.fill_between([], [], [], color='gray', label=\"predicted portion\")\n",
    "                        ax.legend(loc=\"upper left\")\n",
    "            else:\n",
    "\n",
    "                if row == 0:\n",
    "                    ax.legend(loc=\"upper left\", fontsize=8 * (10 / num_lineages) ** 0.8)\n",
    "                elif row == 1:\n",
    "                    if show_case_counts and show_second_legend:\n",
    "                        ax.plot([], \"k--\", color=light, lw=1, label=\"relative #samples\")\n",
    "                        ax.plot([], \"k-\", color=light, lw=0.8, label=\"relative #cases\")\n",
    "                        ax.plot([], lw=0, marker='o', markersize=3, color='gray',\n",
    "                                label=\"observed portion\")\n",
    "                        ax.fill_between([], [], [], color='gray', label=\"predicted portion\")\n",
    "                        ax.legend(loc=\"upper left\",)\n",
    "          \n",
    "    # Setup the date axis correctly\n",
    "    ax.xaxis.set_major_locator(matplotlib.dates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%b %Y\"))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        \n",
    "    return {\n",
    "        'lineages_plotted': plot_lineages_ids_inv_joint,\n",
    "        'colors_dict': colors_dict_export,\n",
    "        'rates': rates,\n",
    "        'ax': ax,\n",
    "        'fig': fig,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial run of last model to find lineages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[-2]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "plot_forecast_results = plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"England\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    additional_lineages = ['BA.1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot for rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = list(range(plot_forecast_results['rates'].shape[0]))\n",
    "ys = np.exp(list(plot_forecast_results['rates']))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x = xs, height = ys, color = list(map(plot_forecast_results['colors_dict'].get, plot_forecast_results['lineages_plotted'])))\n",
    "ax.set_xticks(xs)\n",
    "ax.set_xticklabels(plot_forecast_results['lineages_plotted'].tolist(), rotation=90)\n",
    "ax.set_ylabel('$R_{lineage}/R_A$')\n",
    "plt.savefig('paper/backtesting/barplot_rates_inset.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for Fig S6 (AY.4 and BA.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_figsize_y = 2\n",
    "prediction_figsize_x = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[15]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"England\"],\n",
    "    num_lineages=14,\n",
    "    verbose=False,\n",
    "    filenames =  [f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england.png',\n",
    "                 f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england.pdf'],\n",
    "    figsize_x = 360 / 752 * prediction_figsize_x,\n",
    "    figsize_y = prediction_figsize_y,\n",
    "    additional_lineages = plot_forecast_results['lineages_plotted'],\n",
    "    colors_dict_export = plot_forecast_results['colors_dict'],\n",
    "    auto_select_lineages = False,\n",
    "    legend_out = True,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[27]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"England\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    filenames =  [f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england.png',\n",
    "                 f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england.pdf'],\n",
    "    figsize_x = 528 / 752 * prediction_figsize_x,\n",
    "    figsize_y = prediction_figsize_y,\n",
    "    additional_lineages = plot_forecast_results['lineages_plotted'],\n",
    "    colors_dict_export = plot_forecast_results['colors_dict'],\n",
    "    auto_select_lineages = False,\n",
    "    legend_out = True,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[-3]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"England\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    filenames =  [f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england.png',\n",
    "                 f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england.pdf'],\n",
    "    figsize_x = 738 / 752 * prediction_figsize_x,\n",
    "    figsize_y = prediction_figsize_y,\n",
    "    additional_lineages = plot_forecast_results['lineages_plotted'],\n",
    "    colors_dict_export = plot_forecast_results['colors_dict'],\n",
    "    auto_select_lineages = False,\n",
    "    legend_out = True,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[-2]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"England\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    filenames =  [f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england.png',\n",
    "                 f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england.pdf'],\n",
    "    figsize_x = prediction_figsize_x,\n",
    "    figsize_y = prediction_figsize_y,\n",
    "    additional_lineages = plot_forecast_results['lineages_plotted'],\n",
    "    colors_dict_export = plot_forecast_results['colors_dict'],\n",
    "    auto_select_lineages = False,\n",
    "    legend_out = True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecast2(fit, input_dataset, queries, num_lineages=10, filenames=[], \n",
    "                       verbose=False, data_region = None):\n",
    "    # Convert queries to array if only only string\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "    \n",
    "    # Get dimensions of the model fit (T,P,L) these are probabilities\n",
    "    n_model_periods, n_model_places, n_model_lineages = fit['mean']['probs'].shape\n",
    "    if (verbose):\n",
    "        print('---')\n",
    "        print(f'n_model_periods: {n_model_periods}')\n",
    "        print(f'n_model_places: {n_model_places}')\n",
    "        print(f'n_model_lineages: {n_model_lineages}')\n",
    "    \n",
    "    # Get dimensions of weekly_cases (T,P) these are JHU counts\n",
    "    weekly_cases_fit = fit['weekly_cases']\n",
    "    n_cases_periods, n_cases_places = weekly_cases_fit.shape\n",
    "    if (verbose):\n",
    "        print('---')\n",
    "        print(f'n_cases_periods: {n_cases_periods}')\n",
    "        print(f'n_cases_places: {n_cases_places}')\n",
    "    \n",
    "    # Some checks\n",
    "    assert n_cases_places == n_model_places\n",
    "    assert n_model_periods > n_cases_periods\n",
    "    \n",
    "    # Calculate how many periods are forecasted (i.e. are beyond the input to the model)\n",
    "    n_forecast_steps = n_model_periods - n_cases_periods\n",
    "    if (verbose):\n",
    "        print(f'n_forecast_steps: {n_forecast_steps}')\n",
    "        \n",
    "    # Weekly case counts by time place and clade obtained from the fit\n",
    "    weekly_clades_fit = fit['weekly_clades'] # T, P, C\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'weekly_clades_fit shape: {weekly_clades_fit.shape}')\n",
    "    \n",
    "    # Weekly case counts by time place and clade obtain from the input data\n",
    "    # This has more time point and more regions than the one from the fit\n",
    "    weekly_clades_data = input_dataset['weekly_clades']\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'weekly_clades_data shape: {weekly_clades_data.shape}')\n",
    "    \n",
    "    # Mapping from clades to lineages, a tensor of indexes\n",
    "    # This is valid for both the fit and the input_data\n",
    "    clade_id_to_lineage_id = input_dataset['clade_id_to_lineage_id']\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'clade_id_to_lineage_id length: {len(clade_id_to_lineage_id)}')\n",
    "        \n",
    "    # We don't have clade_id_to_lineage_id in the fit -- it should in principle be the same\n",
    "    \n",
    "    # Summarize the counts of the weekly_clades (from data or fit) to the number of lineages in the model\n",
    "    weekly_lineages_data = weekly_clades_to_lineages(weekly_clades_data, clade_id_to_lineage_id, n_model_lineages)\n",
    "    weekly_lineages_fit = weekly_clades_to_lineages(weekly_clades_fit, clade_id_to_lineage_id, n_model_lineages)\n",
    "    \n",
    "    # Get the probs\n",
    "    probs = fit['mean']['probs']\n",
    "    #probs = plusminus(fit['mean']['probs'], fit['std']['probs']) # [3,T,P,L]\n",
    "    \n",
    "    # Expand weekly_cases_fit (JHU counts) from the model to cover the steps we are forecasting\n",
    "    #padding = 1 + weekly_cases_fit.mean(0, keepdim=True).expand(n_forecast_steps, -1)\n",
    "    #weekly_cases_fit_ = torch.cat([weekly_cases_fit, padding], 0)\n",
    "    # Generate predictions\n",
    "    # Note: For the evaluation maybe we are better off comparing probabilities not counts\n",
    "    #predicted = probs * weekly_cases_fit_[..., None]\n",
    "    \n",
    "    # This is an array of strings listing the locations for the data\n",
    "    location_id_inv_data = input_dataset['location_id_inv']\n",
    "    if (verbose):\n",
    "        print('---')\n",
    "        print(f'location_id_inv_data length: {len(location_id_inv_data)}')\n",
    "    \n",
    "    # This is an array of strings listing the locations for the fit\n",
    "    location_id_inv_fit = fit['location_id_inv']\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'location_id_inv_fit length: {len(location_id_inv_fit)}')\n",
    "    \n",
    "    # Get the location indexes that we want to keep based on query for the data\n",
    "    ids_fit = torch.tensor([i for i, name in enumerate(location_id_inv_fit) if any(q in name for q in queries)])\n",
    "    \n",
    "    # Subset weekly_lineages_fit to those location sum over time and place and get the indices in descending order\n",
    "    lineage_ids_fit = weekly_lineages_fit[:, ids_fit].sum([0, 1]).sort(-1, descending=True).indices\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'lineage_ids_fit shape: {lineage_ids_fit.shape}')\n",
    "    # Keep only the top n number of lineages we want to plot\n",
    "    lineage_ids_fit = lineage_ids_fit[:num_lineages]\n",
    "\n",
    "    # This is problematic without fixing the above permutation\n",
    "    # TODO: Add assert that they are the same set / eliminate code\n",
    "    # Check if order of \n",
    "    lineage_ids_data = lineage_ids_fit[:num_lineages]\n",
    "    \n",
    "    # These are the lineage labels, we can get them from either the fit or the dataset. \n",
    "    # We assume that these are identical and we assert this below\n",
    "    lineage_id_inv_fit = fit['lineage_id_inv']\n",
    "    lineage_id_inv_data = input_dataset['lineage_id_inv']\n",
    "    assert lineage_id_inv_fit == lineage_id_inv_data\n",
    "    \n",
    "    # Get shared locations between full dataset and fit dataset\n",
    "    common_regions = list(set(location_id_inv_fit).intersection(set(location_id_inv_data)))\n",
    "    \n",
    "    if data_region is not None:\n",
    "        common_regions = list(set(common_regions).intersection(set(data_region)))\n",
    "    \n",
    "    # Get indexes of these common regions for each set\n",
    "    common_regions_fit_inv_map = []\n",
    "    common_regions_data_inv_map = []\n",
    "    for r in common_regions:\n",
    "        common_regions_fit_inv_map.append(location_id_inv_fit.index(r))\n",
    "        common_regions_data_inv_map.append(location_id_inv_data.index(r))\n",
    "        \n",
    "    # We want to compare empirical and predicted probabilities for the forecast interval\n",
    "    probs = probs[n_cases_periods:,common_regions_fit_inv_map,:]\n",
    "    \n",
    "    # Subset observed to relevant periods and regions\n",
    "    obs_data = weekly_lineages_data[n_cases_periods:n_cases_periods+n_forecast_steps,common_regions_data_inv_map,:]\n",
    "    empirical_probs = obs_data / obs_data.sum(-1,True).clamp_(min=1e-9)\n",
    "    \n",
    "    # Truncate to availanle data\n",
    "    probs = probs[:empirical_probs.shape[0],]\n",
    "    \n",
    "    # Calculate errors\n",
    "    l1_error = (probs - empirical_probs).abs().sum([-1,-2]) / probs.shape[-2]\n",
    "    l2_error = (probs - empirical_probs).pow(2).sum([-1,-2]).sqrt() / probs.shape[-2]\n",
    "\n",
    "    # consider spearman error\n",
    "    # correlations on the probabilities (average over time)\n",
    "    # precision at k\n",
    "    return {\n",
    "        'L1_error': l1_error,\n",
    "        'L2_error': l2_error,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecast_eval(fits, input_dataset, data_region = None, queries = None):\n",
    "    model_keys = list(fits.keys())\n",
    "    \n",
    "    if not queries:\n",
    "        queries = input_dataset['location_id_inv']\n",
    "\n",
    "    forecast_start_days = []\n",
    "    period_forecast_ahead = []\n",
    "    l1_error = []\n",
    "    l2_error = []\n",
    "    \n",
    "    period_length = 14\n",
    "\n",
    "    for key in model_keys:\n",
    "        forecast_start_day = key[9]\n",
    "        fit_n = fits[key]\n",
    "        # Get forecast error for all independent location ids \n",
    "        forecast_error = evaluate_forecast2(\n",
    "            fit_n, \n",
    "            input_dataset, \n",
    "            queries = queries,\n",
    "            num_lineages=100,\n",
    "            data_region = data_region,\n",
    "        verbose=False)\n",
    "        n_periods_forecast = len(forecast_error['L1_error'].tolist())\n",
    "        forecast_start_days.extend([forecast_start_day] * n_periods_forecast)\n",
    "        period_forecast_ahead.extend(list(range(1,n_periods_forecast+1)))\n",
    "        l1_error.extend(forecast_error['L1_error'].tolist())\n",
    "        l2_error.extend(forecast_error['L2_error'].tolist())\n",
    "        \n",
    "    df1 = pd.DataFrame({\n",
    "    'forecast_start_days': forecast_start_days,\n",
    "    'period_forecast_ahead': period_forecast_ahead, \n",
    "    'l1_error': l1_error, \n",
    "    'l2_error': l2_error})\n",
    "    \n",
    "    df1['day_of_forecast'] = df1['forecast_start_days'] + df1['period_forecast_ahead'] * 14\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## England forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['Europe / United Kingdom / England']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_region_forecast = generate_forecast_eval(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4,3)\n",
    "ax = sns.boxplot(x=\"period_forecast_ahead\", y=\"l1_error\", data=top_region_forecast, color='gray')\n",
    "ax.set(xlabel = '2-week period forecast ahead', ylabel=\"L1 Error\")\n",
    "ax.set_ylim([0.0,2.0])\n",
    "plt.savefig('paper/backtesting/L1_error_barplot_England.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brazil forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = list(x for x in input_dataset['location_id_inv'] if 'Brazil' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_region_forecast = generate_forecast_eval(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4,3)\n",
    "ax = sns.boxplot(x=\"period_forecast_ahead\", y=\"l1_error\", data=top_region_forecast, color='gray')\n",
    "ax.set(xlabel = '2-week period forecast ahead', ylabel=\"L1 Error\")\n",
    "ax.set_ylim([0.0,2.0])\n",
    "plt.savefig('paper/backtesting/L1_error_barplot_Brazil.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = list(x for x in input_dataset['location_id_inv'] if 'Massachusetts' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_region_forecast = generate_forecast_eval(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4,3)\n",
    "ax = sns.boxplot(x=\"period_forecast_ahead\", y=\"l1_error\", data=top_region_forecast, color='gray')\n",
    "ax.set(xlabel = '2-week period forecast ahead', ylabel=\"L1 Error\")\n",
    "ax.set_ylim([0.0,2.0])\n",
    "plt.savefig('paper/backtesting/L1_error_barplot_Massachusetts.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All region forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_region_forecast = generate_forecast_eval(fits, input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"period_forecast_ahead\", y=\"l1_error\", data=all_region_forecast,color='gray')\n",
    "ax.set(xlabel = '2-week period forecast ahead', ylabel=\"L1 Error\")\n",
    "ax.set_ylim([0.0,2.0])\n",
    "plt.savefig('paper/backtesting/L1_error_barplot_all.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Top 100 region forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top covered regions\n",
    "top_region_idx = input_dataset['weekly_clades'].sum([0,2]).sort(-1, descending=True).indices[:100].tolist()\n",
    "regions = list(input_dataset['location_id_inv'][x] for x in top_region_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_region_forecast = generate_forecast_eval(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"period_forecast_ahead\", y=\"l1_error\", data=top_region_forecast, color='gray')\n",
    "ax.set(xlabel = '2-week period forecast ahead', ylabel=\"L1 Error\")\n",
    "ax.set_ylim([0.0,2.0])\n",
    "plt.savefig('paper/backtesting/L1_error_barplot_top100.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 100-200 region forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top covered regions\n",
    "top_region_idx = input_dataset['weekly_clades'].sum([0,2]).sort(-1, descending=True).indices[100:1000].tolist()\n",
    "regions = list(input_dataset['location_id_inv'][x] for x in top_region_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_region_forecast = generate_forecast_eval(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"period_forecast_ahead\", y=\"l1_error\", data=top_region_forecast, color='grey')\n",
    "ax.set(xlabel = '2-week period forecast ahead', ylabel=\"L1 Error\")\n",
    "ax.set_ylim([0.0,2.0])\n",
    "plt.savefig('paper/backtesting/L1_error_barplot_top100-1000.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of forecasting accuracy\n",
    "\n",
    " - What are we trying to do? For a given region and for all models get a % of how often we predict the correct strain n intervals ahead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecast3(fit, input_dataset, queries, num_lineages=10, verbose=False, data_region = None):    \n",
    "    # Convert queries to array if only only string\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "    \n",
    "    # Get dimensions of the model fit (T,P,L) these are probabilities\n",
    "    n_model_periods, n_model_places, n_model_lineages = fit['mean']['probs'].shape\n",
    "    if (verbose):\n",
    "        print('---')\n",
    "        print(f'n_model_periods: {n_model_periods}')\n",
    "        print(f'n_model_places: {n_model_places}')\n",
    "        print(f'n_model_lineages: {n_model_lineages}')\n",
    "    \n",
    "    # Get dimensions of weekly_cases (T,P) these are JHU counts\n",
    "    weekly_cases_fit = fit['weekly_cases']\n",
    "    n_cases_periods, n_cases_places = weekly_cases_fit.shape\n",
    "    if (verbose):\n",
    "        print('---')\n",
    "        print(f'n_cases_periods: {n_cases_periods}')\n",
    "        print(f'n_cases_places: {n_cases_places}')\n",
    "    \n",
    "    # Some checks\n",
    "    assert n_cases_places == n_model_places\n",
    "    assert n_model_periods > n_cases_periods\n",
    "    \n",
    "    # Calculate how many periods are forecasted (i.e. are beyond the input to the model)\n",
    "    n_forecast_steps = n_model_periods - n_cases_periods\n",
    "    if (verbose):\n",
    "        print(f'n_forecast_steps: {n_forecast_steps}')\n",
    "        \n",
    "    # Weekly case counts by time place and clade obtained from the fit\n",
    "    weekly_clades_fit = fit['weekly_clades'] # T, P, C\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'weekly_clades_fit shape: {weekly_clades_fit.shape}')\n",
    "    \n",
    "    # Weekly case counts by time place and clade obtain from the input data\n",
    "    # This has more time point and more regions than the one from the fit\n",
    "    weekly_clades_data = input_dataset['weekly_clades']\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'weekly_clades_data shape: {weekly_clades_data.shape}')\n",
    "    \n",
    "    # Mapping from clades to lineages, a tensor of indexes\n",
    "    # This is valid for both the fit and the input_data\n",
    "    clade_id_to_lineage_id = input_dataset['clade_id_to_lineage_id']\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'clade_id_to_lineage_id length: {len(clade_id_to_lineage_id)}')\n",
    "        \n",
    "    # We don't have clade_id_to_lineage_id in the fit -- it should in principle be the same\n",
    "    \n",
    "    # Summarize the counts of the weekly_clades (from data or fit) to the number of lineages in the model\n",
    "    weekly_lineages_data = weekly_clades_to_lineages(weekly_clades_data, clade_id_to_lineage_id, n_model_lineages)\n",
    "    weekly_lineages_fit = weekly_clades_to_lineages(weekly_clades_fit, clade_id_to_lineage_id, n_model_lineages)\n",
    "    \n",
    "    # Get the probs\n",
    "    probs = fit['mean']['probs']\n",
    "    #probs = plusminus(fit['mean']['probs'], fit['std']['probs']) # [3,T,P,L]\n",
    "    \n",
    "    # Expand weekly_cases_fit (JHU counts) from the model to cover the steps we are forecasting\n",
    "    #padding = 1 + weekly_cases_fit.mean(0, keepdim=True).expand(n_forecast_steps, -1)\n",
    "    #weekly_cases_fit_ = torch.cat([weekly_cases_fit, padding], 0)\n",
    "    # Generate predictions\n",
    "    # Note: For the evaluation maybe we are better off comparing probabilities not counts\n",
    "    #predicted = probs * weekly_cases_fit_[..., None]\n",
    "    \n",
    "    # This is an array of strings listing the locations for the data\n",
    "    location_id_inv_data = input_dataset['location_id_inv']\n",
    "    if (verbose):\n",
    "        print('---')\n",
    "        print(f'location_id_inv_data length: {len(location_id_inv_data)}')\n",
    "    \n",
    "    # This is an array of strings listing the locations for the fit\n",
    "    location_id_inv_fit = fit['location_id_inv']\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'location_id_inv_fit length: {len(location_id_inv_fit)}')\n",
    "    \n",
    "    # Get the location indexes that we want to keep based on query for the data\n",
    "    ids_fit = torch.tensor([i for i, name in enumerate(location_id_inv_fit) if any(q in name for q in queries)])\n",
    "    \n",
    "    # Subset weekly_lineages_fit to those location sum over time and place and get the indices in descending order\n",
    "    lineage_ids_fit = weekly_lineages_fit[:, ids_fit].sum([0, 1]).sort(-1, descending=True).indices\n",
    "    if verbose:\n",
    "        print('---')\n",
    "        print(f'lineage_ids_fit shape: {lineage_ids_fit.shape}')\n",
    "    # Keep only the top n number of lineages we want to plot\n",
    "    lineage_ids_fit = lineage_ids_fit[:num_lineages]\n",
    "\n",
    "    # This is problematic without fixing the above permutation\n",
    "    # TODO: Add assert that they are the same set / eliminate code\n",
    "    # Check if order of \n",
    "    lineage_ids_data = lineage_ids_fit[:num_lineages]\n",
    "    \n",
    "    # These are the lineage labels, we can get them from either the fit or the dataset. \n",
    "    # We assume that these are identical and we assert this below\n",
    "    lineage_id_inv_fit = fit['lineage_id_inv']\n",
    "    lineage_id_inv_data = input_dataset['lineage_id_inv']\n",
    "    assert lineage_id_inv_fit == lineage_id_inv_data\n",
    "    \n",
    "    # Get shared locations between full dataset and fit dataset\n",
    "    common_regions = list(set(location_id_inv_fit).intersection(set(location_id_inv_data)))\n",
    "    \n",
    "    if data_region is not None:\n",
    "        common_regions = list(set(common_regions).intersection(set(data_region)))\n",
    "    \n",
    "    # Get indexes of these common regions for each set\n",
    "    common_regions_fit_inv_map = []\n",
    "    common_regions_data_inv_map = []\n",
    "    for r in common_regions:\n",
    "        common_regions_fit_inv_map.append(location_id_inv_fit.index(r))\n",
    "        common_regions_data_inv_map.append(location_id_inv_data.index(r))\n",
    "        \n",
    "    # We want to compare empirical and predicted probabilities for the forecast interval\n",
    "    probs = probs[n_cases_periods:,common_regions_fit_inv_map,:]\n",
    "    \n",
    "    # Subset observed to relevant periods and regions\n",
    "    obs_data = weekly_lineages_data[n_cases_periods:n_cases_periods+n_forecast_steps,common_regions_data_inv_map,:]\n",
    "    empirical_probs = obs_data / obs_data.sum(-1,True).clamp_(min=1e-9)\n",
    "    \n",
    "    # Truncate to availanle data\n",
    "    probs = probs[:empirical_probs.shape[0]-1,]\n",
    "    \n",
    "    return {\n",
    "        'probs': probs,\n",
    "        'empirical_probs': empirical_probs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecast_eval_percent(fits, input_dataset, data_region = None, queries = None):\n",
    "    model_keys = list(fits.keys())\n",
    "    \n",
    "\n",
    "    match_4wk = []\n",
    "    match_8wk = []\n",
    "    \n",
    "    if queries is None:\n",
    "        queries = input_dataset['location_id_inv']\n",
    "\n",
    "    for key in model_keys:\n",
    "        forecast_start_day = key[9]\n",
    "        fit_n = fits[key]\n",
    "        # Get forecast error for all independent location ids \n",
    "        probs_dict = evaluate_forecast3(\n",
    "            fit_n, \n",
    "            input_dataset, \n",
    "            queries = queries,\n",
    "            num_lineages=101,\n",
    "            data_region = data_region,\n",
    "        verbose=False)\n",
    "        \n",
    "        try:\n",
    "            period_index_4wk = 1\n",
    "            predicted_4wk = probs_dict['probs'][period_index_4wk,:].sum(-2).argmax(0).item()\n",
    "            actual_4wk = probs_dict['empirical_probs'][period_index_4wk,:].sum(-2).argmax(0).item()      \n",
    "\n",
    "            period_index_8wk = 3\n",
    "            predicted_8wk = probs_dict['probs'][period_index_8wk,:].sum(-2).argmax(0).item()\n",
    "            actual_8wk = probs_dict['empirical_probs'][period_index_8wk,].sum(-2).argmax(0).item()\n",
    "\n",
    "            match_4wk.append(predicted_4wk == actual_4wk)\n",
    "            match_8wk.append(predicted_8wk == actual_8wk)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return {\n",
    "        'match_4wk': match_4wk,\n",
    "        'match_8wk': match_8wk,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'USA'\n",
    "regions = list(x for x in input_dataset['location_id'].keys() if query in x)\n",
    "selected_region_forecast = generate_forecast_eval_percent(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_4wk']).sum() / len(selected_region_forecast['match_4wk']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_8wk']).sum() / len(selected_region_forecast['match_8wk']) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'France'\n",
    "regions = list(x for x in input_dataset['location_id'].keys() if query in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_region_forecast = generate_forecast_eval_percent(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_4wk']).sum() / len(selected_region_forecast['match_4wk']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_8wk']).sum() / len(selected_region_forecast['match_8wk']) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'England'\n",
    "regions = list(x for x in input_dataset['location_id'].keys() if query in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_region_forecast = generate_forecast_eval_percent(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_4wk']).sum() / len(selected_region_forecast['match_4wk']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_8wk']).sum() / len(selected_region_forecast['match_8wk']) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brazil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Brazil'\n",
    "regions = list(x for x in input_dataset['location_id'].keys() if query in x)\n",
    "selected_region_forecast = generate_forecast_eval_percent(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_4wk']).sum() / len(selected_region_forecast['match_4wk']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_8wk']).sum() / len(selected_region_forecast['match_8wk']) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Australia'\n",
    "regions = list(x for x in input_dataset['location_id'].keys() if query in x)\n",
    "selected_region_forecast = generate_forecast_eval_percent(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_4wk']).sum() / len(selected_region_forecast['match_4wk']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_8wk']).sum() / len(selected_region_forecast['match_8wk']) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Russia'\n",
    "regions = list(x for x in input_dataset['location_id'].keys() if query in x)\n",
    "selected_region_forecast = generate_forecast_eval_percent(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_4wk']).sum() / len(selected_region_forecast['match_4wk']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(selected_region_forecast['match_8wk']).sum() / len(selected_region_forecast['match_8wk']) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmap of forecast evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from matplotlib.pyplot import cm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecast_eval_2(fits, input_dataset, data_region = None, queries = None):\n",
    "    model_keys = list(fits.keys())\n",
    "    \n",
    "    if not queries:\n",
    "        queries = input_dataset['location_id_inv']\n",
    "\n",
    "    forecast_start_interval = []\n",
    "    period_forecast_ahead = []\n",
    "    l1_error = []\n",
    "    l2_error = []\n",
    "    \n",
    "    period_length = 14\n",
    "\n",
    "    for key in model_keys:\n",
    "        \n",
    "        forecast_start_day = key[9]\n",
    "        fit_n = fits[key]\n",
    "        # Get forecast error for all independent location ids \n",
    "        \n",
    "        forecast_error = evaluate_forecast2(\n",
    "            fit_n, \n",
    "            input_dataset, \n",
    "            queries = queries,\n",
    "            num_lineages=100,\n",
    "            data_region = data_region,\n",
    "        verbose=False)\n",
    "        \n",
    "        forecast_start_interval_t = forecast_start_day // period_length\n",
    "        \n",
    "        n_periods_forecast = len(forecast_error['L1_error'].tolist())\n",
    "        forecast_start_interval.extend([forecast_start_interval_t] * n_periods_forecast)\n",
    "        period_forecast_ahead.extend(list(range(1,n_periods_forecast+1)))\n",
    "        \n",
    "        l1_error.extend(forecast_error['L1_error'].tolist())\n",
    "        l2_error.extend(forecast_error['L2_error'].tolist())\n",
    "        \n",
    "    df1 = pd.DataFrame({\n",
    "        'forecast_start_interval': forecast_start_interval,\n",
    "        'period_forecast_ahead': period_forecast_ahead, \n",
    "        'l1_error': l1_error, \n",
    "        'l2_error': l2_error\n",
    "    })\n",
    "    \n",
    "    df1['period_of_forecast'] = df1['forecast_start_interval'] + df1['period_forecast_ahead']\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap for England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'England'\n",
    "regions = list(x for x in input_dataset['location_id'].keys() if query in x)\n",
    "region_forecast_info = generate_forecast_eval_2(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A date mapping\n",
    "dates = (matplotlib.dates.date2num(mutrans.date_range(region_forecast_info['forecast_start_interval'].max()+12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_forecast_info['forecast_start_interval_time'] = list( map(lambda x: dates[x-1], region_forecast_info['forecast_start_interval']) )\n",
    "region_forecast_info['period_of_forecast_time'] = list( map(lambda x: dates[x-1], region_forecast_info['period_of_forecast']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_forecast_info_pivot = region_forecast_info.pivot(\"forecast_start_interval_time\",\"period_of_forecast_time\",\"l1_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "_data = region_forecast_info_pivot.to_numpy()\n",
    "_cns = np.array(region_forecast_info_pivot.columns)\n",
    "_rns = np.array(list(region_forecast_info_pivot.index))\n",
    "\n",
    "data = np.hstack((np.zeros((_data.shape[0], 1)), _data))\n",
    "cns = np.asarray([_rns[0]] + _cns.tolist())\n",
    "rns = _rns\n",
    "for j in range(min(data.shape)):\n",
    "    data[j, 0] = np.nan\n",
    "    data[j, j] = np.nan\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "im = ax.imshow(data, cmap = cm.plasma, aspect=1, extent = (rns[0], rns[-1], cns[0], cns[-1]), origin='lower', vmin= 0, vmax=2)\n",
    "#ax.invert_yaxis()\n",
    "\n",
    "# ax.set_yticks(ticks = list(range(len(rns))))\n",
    "# ax.set_yticklabels(list(mdates.num2date(x).strftime(\"%d %b %Y\") for x in rns.tolist()), Fontsize = 2)\n",
    "\n",
    "# ax.set_xticks(ticks = list(range(len(cns))))\n",
    "# ax.set_xticklabels(list(mdates.num2date(x).strftime(\"%d %b %Y\") for x in cns.tolist()), Fontsize = 6)\n",
    "\n",
    "ax.yaxis.set_major_locator(matplotlib.dates.MonthLocator())\n",
    "ax.yaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%b %Y\"))\n",
    "\n",
    "ax.xaxis.set_major_locator(matplotlib.dates.MonthLocator())\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%b %Y\"))\n",
    "\n",
    "plt.colorbar(im, orientation = 'horizontal')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "fig.subplots_adjust(bottom = -0.5)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('paper/backtesting/heatmap_England.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap for USA /Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'USA / Massachusetts'\n",
    "regions = list(x for x in input_dataset['location_id'].keys() if query in x)\n",
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_forecast_info = generate_forecast_eval_2(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A date mapping\n",
    "dates = (matplotlib.dates.date2num(mutrans.date_range(region_forecast_info['forecast_start_interval'].max()+12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_forecast_info['forecast_start_interval_time'] = list( map(lambda x: dates[x-1], region_forecast_info['forecast_start_interval']) )\n",
    "region_forecast_info['period_of_forecast_time'] = list( map(lambda x: dates[x-1], region_forecast_info['period_of_forecast']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_forecast_info_pivot = region_forecast_info.pivot(\"forecast_start_interval_time\",\"period_of_forecast_time\",\"l1_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "_data = region_forecast_info_pivot.to_numpy()\n",
    "_cns = np.array(region_forecast_info_pivot.columns)\n",
    "_rns = np.array(list(region_forecast_info_pivot.index))\n",
    "\n",
    "data = np.hstack((np.zeros((_data.shape[0], 1)), _data))\n",
    "cns = np.asarray([_rns[0]] + _cns.tolist())\n",
    "rns = _rns\n",
    "for j in range(min(data.shape)):\n",
    "    data[j, 0] = np.nan\n",
    "    data[j, j] = np.nan\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "im = ax.imshow(data, cmap = cm.plasma, aspect=1, extent = (rns[0], rns[-1], cns[0], cns[-1]), origin='lower', vmin= 0, vmax=2)\n",
    "#ax.invert_yaxis()\n",
    "\n",
    "# ax.set_yticks(ticks = list(range(len(rns))))\n",
    "# ax.set_yticklabels(list(mdates.num2date(x).strftime(\"%d %b %Y\") for x in rns.tolist()), Fontsize = 2)\n",
    "\n",
    "# ax.set_xticks(ticks = list(range(len(cns))))\n",
    "# ax.set_xticklabels(list(mdates.num2date(x).strftime(\"%d %b %Y\") for x in cns.tolist()), Fontsize = 6)\n",
    "\n",
    "ax.yaxis.set_major_locator(matplotlib.dates.MonthLocator())\n",
    "ax.yaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%b %Y\"))\n",
    "\n",
    "ax.xaxis.set_major_locator(matplotlib.dates.MonthLocator())\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%b %Y\"))\n",
    "\n",
    "plt.colorbar(im, orientation = 'horizontal')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "fig.subplots_adjust(bottom = -0.5)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('paper/backtesting/heatmap_USA_Mass.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap for Brazil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Brazil'\n",
    "regions = list(x for x in input_dataset['location_id'].keys() if query in x)\n",
    "region_forecast_info = generate_forecast_eval_2(fits, input_dataset, data_region = regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A date mapping\n",
    "dates = (matplotlib.dates.date2num(mutrans.date_range(region_forecast_info['forecast_start_interval'].max()+12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_forecast_info['forecast_start_interval_time'] = list( map(lambda x: dates[x-1], region_forecast_info['forecast_start_interval']) )\n",
    "region_forecast_info['period_of_forecast_time'] = list( map(lambda x: dates[x-1], region_forecast_info['period_of_forecast']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_forecast_info_pivot = region_forecast_info.pivot(\"forecast_start_interval_time\",\"period_of_forecast_time\",\"l1_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "_data = region_forecast_info_pivot.to_numpy()\n",
    "_cns = np.array(region_forecast_info_pivot.columns)\n",
    "_rns = np.array(list(region_forecast_info_pivot.index))\n",
    "\n",
    "data = np.hstack((np.zeros((_data.shape[0], 1)), _data))\n",
    "cns = np.asarray([_rns[0]] + _cns.tolist())\n",
    "rns = _rns\n",
    "for j in range(min(data.shape)):\n",
    "    data[j, 0] = np.nan\n",
    "    data[j, j] = np.nan\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "im = ax.imshow(data, cmap = cm.plasma, aspect=1, extent = (rns[0], rns[-1], cns[0], cns[-1]), origin='lower', vmin= 0, vmax=2)\n",
    "#ax.invert_yaxis()\n",
    "\n",
    "# ax.set_yticks(ticks = list(range(len(rns))))\n",
    "# ax.set_yticklabels(list(mdates.num2date(x).strftime(\"%d %b %Y\") for x in rns.tolist()), Fontsize = 2)\n",
    "\n",
    "# ax.set_xticks(ticks = list(range(len(cns))))\n",
    "# ax.set_xticklabels(list(mdates.num2date(x).strftime(\"%d %b %Y\") for x in cns.tolist()), Fontsize = 6)\n",
    "\n",
    "ax.yaxis.set_major_locator(matplotlib.dates.MonthLocator())\n",
    "ax.yaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%b %Y\"))\n",
    "\n",
    "ax.xaxis.set_major_locator(matplotlib.dates.MonthLocator())\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%b %Y\"))\n",
    "\n",
    "plt.colorbar(im, orientation = 'horizontal')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "fig.subplots_adjust(bottom = -0.5)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('paper/backtesting/heatmap_Brazil.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corresponding Region Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[-1]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "england_for_heatmap = plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"England\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    figsize_x = 7,\n",
    "    figsize_y = prediction_figsize_y,\n",
    "    additional_lineages = plot_forecast_results['lineages_plotted'],\n",
    "    #colors_dict_export = plot_forecast_results['colors_dict'],\n",
    "    auto_select_lineages = True,\n",
    "    legend_out = True\n",
    ");\n",
    "start_date = matplotlib.dates.date2num(np.datetime64('2020-05-01'))\n",
    "end_date = matplotlib.dates.date2num(np.datetime64('2021-12-01'))\n",
    "england_for_heatmap['ax'].set_xlim((start_date, end_date))\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england_for_heatmap.pdf',bbox_inches = 'tight')\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england_for_heatmap.png',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[-1]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "england_for_heatmap = plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"England\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    figsize_x = 7,\n",
    "    figsize_y = prediction_figsize_y,\n",
    "    additional_lineages = plot_forecast_results['lineages_plotted'],\n",
    "    #colors_dict_export = plot_forecast_results['colors_dict'],\n",
    "    auto_select_lineages = True,\n",
    "    legend_out = True,\n",
    "    show_legend = False,\n",
    ");\n",
    "start_date = matplotlib.dates.date2num(np.datetime64('2020-05-01'))\n",
    "end_date = matplotlib.dates.date2num(np.datetime64('2021-12-01'))\n",
    "england_for_heatmap['ax'].set_xlim((start_date, end_date))\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england_for_heatmap_nolegend.png',bbox_inches = 'tight')\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_england_for_heatmap_nolegend.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brazil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[-1]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "england_for_heatmap = plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"Brazil\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    figsize_x = 7,\n",
    "    figsize_y = prediction_figsize_y,\n",
    "    additional_lineages = plot_forecast_results['lineages_plotted'],\n",
    "    #colors_dict_export = plot_forecast_results['colors_dict'],\n",
    "    auto_select_lineages = True,\n",
    "    legend_out = True\n",
    ");\n",
    "start_date = matplotlib.dates.date2num(np.datetime64('2020-05-01'))\n",
    "end_date = matplotlib.dates.date2num(np.datetime64('2021-12-01'))\n",
    "england_for_heatmap['ax'].set_xlim((start_date, end_date))\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_brazil_for_heatmap.pdf',bbox_inches = 'tight')\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_brazil_for_heatmap.png',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[-1]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "england_for_heatmap = plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"Brazil\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    figsize_x = 7,\n",
    "    figsize_y = prediction_figsize_y,\n",
    "    additional_lineages = plot_forecast_results['lineages_plotted'],\n",
    "    #colors_dict_export = plot_forecast_results['colors_dict'],\n",
    "    auto_select_lineages = True,\n",
    "    legend_out = True,\n",
    "    show_legend = False,\n",
    ");\n",
    "start_date = matplotlib.dates.date2num(np.datetime64('2020-05-01'))\n",
    "end_date = matplotlib.dates.date2num(np.datetime64('2021-12-01'))\n",
    "england_for_heatmap['ax'].set_xlim((start_date, end_date))\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_brazil_for_heatmap_nolegend.png',bbox_inches = 'tight')\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_brazil_for_heatmap_nolegend.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USA / Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[-1]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "england_for_heatmap = plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"Massachusetts\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    figsize_x = 7,\n",
    "    figsize_y = prediction_figsize_y,\n",
    "    additional_lineages = plot_forecast_results['lineages_plotted'],\n",
    "    #colors_dict_export = plot_forecast_results['colors_dict'],\n",
    "    auto_select_lineages = True,\n",
    "    legend_out = True\n",
    ");\n",
    "start_date = matplotlib.dates.date2num(np.datetime64('2020-05-01'))\n",
    "end_date = matplotlib.dates.date2num(np.datetime64('2021-12-01'))\n",
    "england_for_heatmap['ax'].set_xlim((start_date, end_date))\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_Massachusetts_for_heatmap.pdf',bbox_inches = 'tight')\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_Massachusetts_for_heatmap.png',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(fits.keys())[-1]\n",
    "print(k[9])\n",
    "fit_n = fits[k]\n",
    "england_for_heatmap = plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"Massachusetts\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    figsize_x = 7,\n",
    "    figsize_y = prediction_figsize_y,\n",
    "    additional_lineages = plot_forecast_results['lineages_plotted'],\n",
    "    #colors_dict_export = plot_forecast_results['colors_dict'],\n",
    "    auto_select_lineages = True,\n",
    "    legend_out = True,\n",
    "    show_legend = False,\n",
    ");\n",
    "start_date = matplotlib.dates.date2num(np.datetime64('2020-05-01'))\n",
    "end_date = matplotlib.dates.date2num(np.datetime64('2021-12-01'))\n",
    "england_for_heatmap['ax'].set_xlim((start_date, end_date))\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_Massachusetts_for_heatmap_nolegend.png',bbox_inches = 'tight')\n",
    "plt.savefig(f'{forecast_dir_prefix}/backtesting_day_{k[9]}_early_prediction_Massachusetts_for_heatmap_nolegend.pdf',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_analysis_fits = torch.load(\"results/mutrans.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_analysis_fits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(main_analysis_fits.keys())[0]\n",
    "print(k[9])\n",
    "fit_n = main_analysis_fits[k]\n",
    "plot_forecast_results = plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"USA\"],\n",
    "    num_lineages=1,\n",
    "    verbose=False,\n",
    "    additional_lineages = ['BA.1.1'],\n",
    "    legend_out=True,\n",
    "    show_case_counts = False,\n",
    "    filenames = [\"paper/Figure_S3.pdf\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(main_analysis_fits.keys())[0]\n",
    "print(k[9])\n",
    "fit_n = main_analysis_fits[k]\n",
    "plot_forecast_results = plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"USA\",\"France\",\"England\",\"Brazil\",\"Australia\",\"Russia\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    additional_lineages = ['BA.2'],\n",
    "    legend_out=True,\n",
    "    show_case_counts = False,\n",
    "    filenames = [\"paper/Figure_S3.pdf\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(main_analysis_fits.keys())[0]\n",
    "print(k[9])\n",
    "fit_n = main_analysis_fits[k]\n",
    "plot_forecast_results = plot_forecast2(\n",
    "    fit_n, \n",
    "    input_dataset, \n",
    "    queries=[\"USA\",\"France\",\"England\",\"Brazil\",\"Australia\",\"Russia\"],\n",
    "    num_lineages=13,\n",
    "    verbose=False,\n",
    "    additional_lineages = ['BA.1','BA.2'],\n",
    "    legend_out=True,\n",
    "    show_case_counts = True,\n",
    "    filenames = [\"paper/Figure_S3_withcases.pdf\"],\n",
    "    show_second_legend = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = list(range(plot_forecast_results['rates'].shape[0]))\n",
    "ys = np.exp(list(plot_forecast_results['rates']))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x = xs, height = ys, color = list(map(plot_forecast_results['colors_dict'].get, plot_forecast_results['lineages_plotted'])))\n",
    "ax.set_xticks(xs)\n",
    "ax.set_xticklabels(plot_forecast_results['lineages_plotted'].tolist(), rotation=90)\n",
    "ax.set_ylabel('$R_{lineage}/R_A$')\n",
    "plt.savefig('paper/barplot_rates_inset_for_S3.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot for Asia for the reviewer response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_analysis_fits = torch.load(\"results/mutrans.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_keys = main_analysis_fits[list(main_analysis_fits.keys())[0]]['location_id'].keys()\n",
    "fit_n = main_analysis_fits[list(main_analysis_fits.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(x for x in location_keys if 'Asia' in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast_results = plot_forecast2(\n",
    "    fit_n,\n",
    "    input_dataset,\n",
    "    num_lineages = 10,\n",
    "    legend_out=True,show_second_legend=False,\n",
    "    queries=['Asia'],\n",
    "    filenames = ['paper/forecast_Asia.pdf','paper/forecast_Asia.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate selected per-region forecasts showing BA.2 rising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_analysis_fits = torch.load(\"results/mutrans.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_keys = main_analysis_fits[list(main_analysis_fits.keys())[0]]['location_id'].keys()\n",
    "fit_n = main_analysis_fits[list(main_analysis_fits.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast_results = plot_forecast2(\n",
    "    fit_n,\n",
    "    input_dataset,\n",
    "    num_lineages = 10,\n",
    "    legend_out=True,show_second_legend=False,\n",
    "    queries=['Denmark','South Africa','India'],\n",
    "    filenames = ['paper/forecast_Denmark_SouthAfrica_India.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = list(range(plot_forecast_results['rates'].shape[0]))\n",
    "ys = np.exp(list(plot_forecast_results['rates']))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x = xs, height = ys, color = list(map(plot_forecast_results['colors_dict'].get, plot_forecast_results['lineages_plotted'])))\n",
    "ax.set_xticks(xs)\n",
    "ax.set_xticklabels(plot_forecast_results['lineages_plotted'].tolist(), rotation=90)\n",
    "ax.set_ylabel('$R_{lineage}/R_A$')\n",
    "plt.savefig('paper/barplot_rates_inset_for_forecast_Denmark_SouthAfrica_India.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction accuracy vs Region Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecast3(fit, input_dataset, filenames=[]):\n",
    "\n",
    "    n_model_periods, n_model_places, n_model_lineages = fit['mean']['probs'].shape\n",
    "\n",
    "    weekly_cases_fit = fit['weekly_cases']\n",
    "    n_cases_periods, n_cases_places = weekly_cases_fit.shape\n",
    "    n_forecast_steps = n_model_periods - n_cases_periods\n",
    "    \n",
    "    weekly_clades_fit = fit['weekly_clades'] # T, P, C\n",
    "    weekly_clades_data = input_dataset['weekly_clades']\n",
    "    clade_id_to_lineage_id = input_dataset['clade_id_to_lineage_id']\n",
    "    \n",
    "    # Summarize the counts of the weekly_clades (from data or fit) to the number of lineages in the model\n",
    "    weekly_lineages_data = weekly_clades_to_lineages(weekly_clades_data, clade_id_to_lineage_id, n_model_lineages)\n",
    "    weekly_lineages_fit = weekly_clades_to_lineages(weekly_clades_fit, clade_id_to_lineage_id, n_model_lineages)\n",
    "    \n",
    "    # This is an array of strings listing the locations for the data\n",
    "    location_id_inv_data = input_dataset['location_id_inv']\n",
    "\n",
    "    # This is an array of strings listing the locations for the fit\n",
    "    location_id_inv_fit = fit['location_id_inv']\n",
    "    lineage_id_inv_fit = fit['lineage_id_inv']\n",
    "    lineage_id_inv_data = input_dataset['lineage_id_inv']\n",
    "        \n",
    "    location_id_fit = fit['location_id']\n",
    "    location_id_data = input_dataset['location_id']\n",
    "\n",
    "    common_regions = list(set(location_id_inv_fit).intersection(set(location_id_inv_data)))\n",
    "    common_regions_fit_inv_map = list(map(location_id_fit.get, common_regions))\n",
    "    common_regions_data_inv_map = list(map(location_id_data.get, common_regions))\n",
    "        \n",
    "    # We want to compare empirical and predicted probabilities for the forecast interval\n",
    "    probs = fit['mean']['probs']\n",
    "    probs = probs[n_cases_periods:, common_regions_fit_inv_map, :]\n",
    "    \n",
    "    # Subset observed to relevant periods and regions\n",
    "    obs_data = weekly_lineages_data[n_cases_periods:n_cases_periods+n_forecast_steps, common_regions_data_inv_map,:]\n",
    "    empirical_probs = obs_data / obs_data.sum(-1,True).clamp_(min=1e-9)\n",
    "    \n",
    "    # Calculate errors\n",
    "    l1_error = (probs[:empirical_probs.shape[0],] - empirical_probs).abs().sum(-1)\n",
    "    l2_error = (probs[:empirical_probs.shape[0],] - empirical_probs).abs().pow(2).sum(-1).sqrt()\n",
    "\n",
    "    return {\n",
    "        'L1_error': l1_error, # T x P\n",
    "        'L2_error': l2_error, # T x P\n",
    "        'common_regions': common_regions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_col = []\n",
    "fit_days = []\n",
    "L1_error = []\n",
    "\n",
    "for fit_key in tqdm.tqdm(fits.keys()):\n",
    "    fit_n = fits[fit_key]\n",
    "    days = fit_key[9]\n",
    "\n",
    "    forecast_error = evaluate_forecast3(\n",
    "        fit_n, \n",
    "        input_dataset)\n",
    "        \n",
    "    fit_days.extend(list(days for i in range(len(forecast_error['common_regions']))))\n",
    "    regions_col.extend(forecast_error['common_regions'])\n",
    "    L1_error.extend(forecast_error['L1_error'][0].tolist()) # The 2 week forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.DataFrame({\n",
    "    'fit_days': fit_days,\n",
    "    'region': regions_col,\n",
    "    'L1_error': L1_error\n",
    "})\n",
    "total_region_coverage = pd.DataFrame(\n",
    "    {\n",
    "        'region': last_fit['location_id_inv'],\n",
    "        'total_counts': last_fit['weekly_clades'].sum([0,2]).tolist()\n",
    "    }\n",
    ")\n",
    "merged = error_df.merge(total_region_coverage, on = 'region')\n",
    "merged['total_counts_per_day'] = merged['total_counts'] / merged['fit_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"total_counts_per_day_p1\"] = merged[\"total_counts_per_day\"] + 1e-2\n",
    "merged[\"L1_error_p1\"] = merged[\"L1_error\"] + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    merged,\n",
    "    x='total_counts_per_day_p1', \n",
    "    y='L1_error_p1', \n",
    "    log_scale=(True, False), \n",
    "    bins=10, \n",
    "    thresh=None).set(title='All Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    merged[merged['region'].str.contains('Asia')],\n",
    "    x='total_counts_per_day_p1', \n",
    "    y='L1_error_p1', \n",
    "    log_scale=(True, False), \n",
    "    bins=10, \n",
    "    thresh=None).set(title='Asia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    merged[merged['region'].str.contains('USA')],\n",
    "    x='total_counts_per_day_p1', \n",
    "    y='L1_error_p1', \n",
    "    log_scale=(True, False), \n",
    "    bins=10, \n",
    "    thresh=None).set(title='USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    merged[merged['region'].str.contains('United Kingdom')],\n",
    "    x='total_counts_per_day_p1', \n",
    "    y='L1_error_p1', \n",
    "    log_scale=(True, False), \n",
    "    bins=10, \n",
    "    thresh=None).set(title='United Kingdom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    merged[merged['region'].str.contains('Europe')],\n",
    "    x='total_counts_per_day_p1', \n",
    "    y='L1_error_p1', \n",
    "    log_scale=(True, False), \n",
    "    bins=10, \n",
    "    thresh=None).set(title='Europe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    merged[merged['region'].str.contains('Denmark')],\n",
    "    x='total_counts_per_day_p1', \n",
    "    y='L1_error_p1', \n",
    "    log_scale=(True, False), \n",
    "    bins=10, \n",
    "    thresh=None).set(title='Denmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    merged[merged['region'].str.contains('India')],\n",
    "    x='total_counts_per_day_p1', \n",
    "    y='L1_error_p1', \n",
    "    log_scale=(True, False), \n",
    "    bins=10, \n",
    "    thresh=None).set(title='India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    merged[merged['region'].str.contains('South Africa')],\n",
    "    x='total_counts_per_day_p1', \n",
    "    y='L1_error_p1', \n",
    "    log_scale=(True, False), \n",
    "    bins=10, \n",
    "    thresh=None).set(title='South Africa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Asia sub-regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast_results = plot_forecast2(\n",
    "    fit_n,\n",
    "    input_dataset,\n",
    "    num_lineages = 10,\n",
    "    legend_out=True,show_second_legend=False,\n",
    "    queries=['Asia / Japan', 'Asia / India', 'Asia / Myanmar', 'Asia / Pakistan / Multan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction Accuracy Vs Sample Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
