{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring GISAID alignment-free clustering\n",
    "\n",
    "This explores a low-dimensional embedding constructed via AMS sketches of k-mers. To run this notebook, first get GISAID data (sign agreement, set up feed, ...), then run\n",
    "```sh\n",
    "python preprocess_gisaid.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from pyrophylo.cluster import ClockSketcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.load(\"results/gisaid.sketch.pt\")\n",
    "sketch = result[\"sketch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sketch.count.float().numpy(), bins=100)\n",
    "plt.title(f\"mean k-mer count = {sketch.count.float().mean():0.1f}\")\n",
    "plt.yscale(\"log\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = sketch[torch.randperm(len(sketch))[:200]]\n",
    "sketcher = ClockSketcher(20)\n",
    "diffs, std = sketcher.estimate_set_difference(batch, batch)\n",
    "d = diffs.reshape(-1)\n",
    "l = (batch.count[:, None] - batch.count).float()\n",
    "\n",
    "plt.scatter(l, d, lw=0, alpha=0.01)\n",
    "plt.plot([0, l.max()], [0, l.max()], \"k--\", lw=1)\n",
    "plt.plot([l.min(), 0], [0, 0], \"k--\", lw=1)\n",
    "plt.xlabel(\"|x| - |y|\")\n",
    "plt.ylabel(r\"|x \\ y|\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = torch.load(\"results/gisaid.cluster.pt\")\n",
    "full_clusters = clustering[\"full_clusters\"]\n",
    "full_weights = clustering[\"full_weights\"]\n",
    "clusters = clustering[\"clusters\"]\n",
    "weights = clustering[\"weights\"]\n",
    "class_probs = clustering[\"class_probs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(full_weights)\n",
    "plt.xlabel(\"cluster rank\")\n",
    "plt.ylabel(\"cluster size\")\n",
    "plt.yscale(\"log\")\n",
    "p = full_weights / full_weights.sum()\n",
    "perplexity = p.log().neg().mul(p).sum().exp()\n",
    "plt.title(f\"weight = {full_weights.sum():0.1f}, perplexity = {perplexity:0.2f}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_result = torch.load(\"results/gisaid.sketch.pt\")\n",
    "day = torch.tensor(sketch_result[\"columns\"][\"day\"], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(day.numpy(), bins=200)\n",
    "plt.yscale(\"log\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain = class_probs.max(-1).indices\n",
    "# strain = class_probs.multinomial(1)  # Extremely slow.\n",
    "week = day // 7\n",
    "num_weeks = 1 + int(week.max())\n",
    "num_strains = class_probs.size(-1)\n",
    "counts = torch.zeros(num_strains, num_weeks)\n",
    "i = strain * num_weeks + week\n",
    "counts.reshape(-1).scatter_add_(0, i, torch.tensor(1.).expand_as(i))\n",
    "\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "plt.plot(counts.T, lw=1)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"week after 2019-12-01\")\n",
    "plt.ylabel(\"samples / week\")\n",
    "plt.title(f\"{num_strains} clusters\")\n",
    "plt.xlim(0, num_weeks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBSOLETE GMM model based on sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import AutoDelta, init_to_sample\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "from pyro.ops.indexing import Vindex\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "data = clustering[\"soft_hashes\"].clone()\n",
    "data -= data.mean(0)\n",
    "data /= data.std(0)\n",
    "\n",
    "@config_enumerate\n",
    "def model(num_clusters, data):\n",
    "    loc = pyro.sample(\"loc\",\n",
    "                      dist.Normal(0, 1).expand([num_clusters, data.size(-1)]).to_event(2))\n",
    "    scale = pyro.sample(\"scale\", dist.LogNormal(-1, 1))\n",
    "    weights = pyro.sample(\"weights\", dist.Dirichlet(torch.full((num_clusters,), 3.)))\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=256) as ind:\n",
    "        c = pyro.sample(\"component\", dist.Categorical(weights))\n",
    "        pyro.sample(\"locs\", dist.Normal(loc[c], scale).to_event(1),\n",
    "                    obs=data[ind])\n",
    "\n",
    "num_clusters = 10\n",
    "guide = AutoDelta(poutine.block(model, hide=[\"component\"]),\n",
    "                  init_loc_fn=init_to_sample)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(20201223)\n",
    "svi = SVI(model, guide, Adam({\"lr\": 0.1}), TraceEnum_ELBO(max_plate_nesting=1))\n",
    "losses = []\n",
    "for step in range(1001):\n",
    "    loss = svi.step(num_clusters, data) / data.numel()\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step: >4d} loss = {loss:0.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    median = guide.median()\n",
    "print(median[\"scale\"])\n",
    "print(median[\"loc\"][:, 0].data.numpy())\n",
    "print(median[\"weights\"].data.sort(0).values.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clusters = median[\"loc\"].data\n",
    "u = umap.UMAP().fit_transform(clusters)\n",
    "plt.scatter(u[:, 0], u[:, 1], lw=0, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
