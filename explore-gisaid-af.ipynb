{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring GISAID alignment-free clustering\n",
    "\n",
    "This explores a low-dimensional embedding constructed via AMS sketches of k-mers. To run this notebook, first get GISAID data (sign agreement, set up feed, ...), then run\n",
    "```sh\n",
    "python preprocess_gisaid.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import umap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"results/gisaid.cluster.pt\"\n",
    "clustering = torch.load(filename)\n",
    "clusters = clustering[\"clusters\"]\n",
    "cc_distances = clustering[\"cc_distances\"]\n",
    "hc_distances = clustering[\"hc_distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = hc_distances.mul(-3.5)\n",
    "probs = (probs - probs.logsumexp(-1, True)).exp()\n",
    "perplexity = probs.log().mul(probs).neg().sum(-1).exp()\n",
    "best = hc_distances.min(-1).indices\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.scatter(best, perplexity, lw=0, alpha=0.1)\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.xlabel(\"cluster rank\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = clustering[\"soft_hashes\"]\n",
    "h = h[torch.randperm(len(h))[:10000]]\n",
    "mean = h.mean(0)\n",
    "std = h.std(0)\n",
    "bits = h.size(-1)\n",
    "\n",
    "rows = 6\n",
    "fig, axes = plt.subplots(rows, (bits // 2 + rows - 1) // cols, figsize=(12, 12), dpi=200)\n",
    "axes = [a for a_ in axes for a in a_]\n",
    "i = 0\n",
    "for ax in axes:\n",
    "    j = i + 1\n",
    "    if j >= bits:\n",
    "        break\n",
    "    x, y = h[:, i], h[:, j]\n",
    "    ax.scatter(x, y, lw=0, alpha=0.01)\n",
    "    ax.set_xlim(mean[i] - 3 * std[i], mean[i] + 3 * std[i])\n",
    "    ax.set_ylim(mean[j] - 3 * std[j], mean[j] + 3 * std[j])\n",
    "    i += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "h = clustering[\"soft_hashes\"]\n",
    "h = h[torch.randperm(len(h))[:50000]]\n",
    "u = umap.UMAP().fit_transform(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(u[:, 0], u[:, 1], lw=0, alpha=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import AutoDelta, init_to_sample\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, config_enumerate\n",
    "from pyro.optim import Adam\n",
    "from pyro.ops.indexing import Vindex\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "data = clustering[\"soft_hashes\"].clone()\n",
    "data -= data.mean(0)\n",
    "data /= data.std(0)\n",
    "\n",
    "@config_enumerate\n",
    "def model(num_clusters, data):\n",
    "    loc = pyro.sample(\"loc\",\n",
    "                      dist.Normal(0, 1).expand([num_clusters, data.size(-1)]).to_event(2))\n",
    "    scale = pyro.sample(\"scale\", dist.LogNormal(-1, 1))\n",
    "    weights = pyro.sample(\"weights\", dist.Dirichlet(torch.full((num_clusters,), 3.)))\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=256) as ind:\n",
    "        c = pyro.sample(\"component\", dist.Categorical(weights))\n",
    "        pyro.sample(\"locs\", dist.Normal(loc[c], scale).to_event(1),\n",
    "                    obs=data[ind])\n",
    "\n",
    "num_clusters = 10\n",
    "guide = AutoDelta(poutine.block(model, hide=[\"component\"]),\n",
    "                  init_loc_fn=init_to_sample)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(20201223)\n",
    "svi = SVI(model, guide, Adam({\"lr\": 0.1}), TraceEnum_ELBO(max_plate_nesting=1))\n",
    "losses = []\n",
    "for step in range(1001):\n",
    "    loss = svi.step(num_clusters, data) / data.numel()\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step: >4d} loss = {loss:0.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    median = guide.median()\n",
    "print(median[\"scale\"])\n",
    "print(median[\"loc\"][:, 0].data.numpy())\n",
    "print(median[\"weights\"].data.sort(0).values.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clusters = median[\"loc\"].data\n",
    "u = umap.UMAP().fit_transform(clusters)\n",
    "plt.scatter(u[:, 0], u[:, 1], lw=0, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
