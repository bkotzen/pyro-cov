{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling relative strain prevalence\n",
    "\n",
    "This notebook explores Pyro models for forecasting relative strain prevalance based on GISAID sequence data labeled with Pangolin lineage and amino acid mutation features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.distributions import constraints\n",
    "from pyro.infer import SVI, Trace_ELBO, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoDelta, AutoNormal, init_to_median\n",
    "from pyro.infer.reparam import HaarReparam\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "from pyrocov import pangolin\n",
    "from pyrocov.geo import gisaid_to_jhu_location, parse_date, pd_to_torch, read_csv\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "We'll use all GISAID data. You'll need to request a feed from gisaid.org, download, then run\n",
    "```sh\n",
    "python preprocess_gisaid.py\n",
    "nextclade --input-fasta results/gisaid.subset.fasta \\\n",
    "          --output-tsv results/gisaid.subset.tsv\n",
    "python featurize_nextclade.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/gisaid.columns.pkl\", \"rb\") as f:\n",
    "    columns = pickle.load(f)\n",
    "print(\"Training on {} rows with columns:\".format(len(columns[\"day\"])))\n",
    "print(\", \".join(columns.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineages = list(map(pangolin.compress, columns[\"lineage\"]))\n",
    "print(f\"Top 12 of {len(set(lineages))} lineages\")\n",
    "print(\"-\" * 30)\n",
    "for lineage, count in Counter(lineages).most_common(12):\n",
    "    print(f\"{count: >10d} {lineage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = columns[\"location\"]\n",
    "print(f\"Top 12 of {len(set(locations))} GISAID locations\")\n",
    "print(\"-\" * 30)\n",
    "for location, count in Counter(locations).most_common(12):\n",
    "    print(f\"{count: >10d} {location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_features = torch.load(\"results/nextclade.features.pt\")\n",
    "print(aa_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(aa_features[\"features\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll account for epidemiological dynamics in the form of random drift on top of our logistic growth model. Since random drift is inversely proportional to the local number of infections, we'll need a new data source for the number of infections in each region. We'll use JHU's confirmed case counts time series as a proxy for the number of total infections in each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cases_df = read_csv(\"time_series_covid19_confirmed_US.csv\")\n",
    "global_cases_df = read_csv(\"time_series_covid19_confirmed_global.csv\")\n",
    "case_data = torch.cat([\n",
    "    pd_to_torch(us_cases_df, columns=slice(11, None)),\n",
    "    pd_to_torch(global_cases_df, columns=slice(4, None)),\n",
    "]).T\n",
    "sample_region, sample_matrix, region_tuples = gisaid_to_jhu_location(\n",
    "    columns, us_cases_df, global_cases_df\n",
    ")\n",
    "num_gisaid_regions, num_jhu_regions = sample_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(case_data.shape)\n",
    "print(sample_region.shape)\n",
    "print(sample_matrix.shape)\n",
    "print(len(set(columns['location'])))\n",
    "print(len(columns['location']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating regions\n",
    "\n",
    "We'll aggregate locations by either US state, UK regions, or other country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = aa_features[\"features\"]\n",
    "lineages = list(map(pangolin.compress, columns[\"lineage\"]))\n",
    "lineage_id_inv = list(map(pangolin.compress, aa_features[\"lineages\"]))\n",
    "lineage_id = {k: i for i, k in enumerate(lineage_id_inv)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_data = Counter()\n",
    "location_id = {}\n",
    "quotient = {}\n",
    "for day, location, lineage in zip(columns[\"day\"], columns[\"location\"], lineages):\n",
    "    parts = location.split(\"/\")\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    parts = [p.strip() for p in parts[:3]]\n",
    "    if parts[1] not in (\"USA\", \"United Kingdom\"):\n",
    "        parts = parts[:2]\n",
    "    quotient[location] = \" / \".join(parts)\n",
    "    p = location_id.setdefault(quotient[location], len(location_id))\n",
    "    s = lineage_id[lineage]\n",
    "    t = day // 7\n",
    "    sparse_data[t, p, s] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1 + max(columns[\"day\"]) // 7\n",
    "P = len(location_id)\n",
    "S = len(lineage_id)\n",
    "weekly_strains = torch.zeros(T, P, S)\n",
    "for (t, p, s), n in sparse_data.items():\n",
    "    weekly_strains[t, p, s] = n\n",
    "print(weekly_strains.shape, weekly_strains.shape.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotient_matrix = torch.zeros(len(location_id), num_jhu_regions)\n",
    "gisaid_id = {k: i for i, k in enumerate(sorted(quotient))}\n",
    "for g, q in quotient.items():\n",
    "    quotient_matrix[location_id[q]] += sample_matrix[gisaid_id[g]]\n",
    "count_data = case_data @ quotient_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert from daily to weekly observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = parse_date(\"12/01/19\")\n",
    "jhu_start_date = parse_date(us_cases_df.columns[11])\n",
    "assert start_date < jhu_start_date\n",
    "dt = (jhu_start_date - start_date).days\n",
    "weekly_cases = torch.zeros(T, P)\n",
    "for w in range(7):\n",
    "    t0 = (w + dt) // 7\n",
    "    w_data = count_data[w::7]\n",
    "    w_data = w_data[:len(weekly_cases[t0:t0+len(w_data)])]  # allow jhu after end of gisaid\n",
    "    weekly_cases[t0:t0+len(w_data)] += w_data\n",
    "print(weekly_cases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristically estimate the number of infections. This need not be very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_rate = 0.05\n",
    "weekly_infections = torch.maximum(weekly_cases, weekly_strains.sum(-1)) / response_rate\n",
    "weekly_infections.clamp_(min=1 / response_rate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Pyro model\n",
    "\n",
    "Our first model is a logistic growth model with a Dirichlet-multinomial likelihood (the multivariate generalization of negative binomial likelihood). This ignores any spatial structure across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(weekly_strains, weekly_infections, features, feature_scale=1.0,\n",
    "          process_noise=False):\n",
    "    assert weekly_strains.shape[-1] == features.shape[0]\n",
    "    assert weekly_infections.shape == weekly_strains.shape[:2]\n",
    "    T, P, S = weekly_strains.shape\n",
    "    S, F = features.shape\n",
    "    time_plate = pyro.plate(\"time\", T, dim=-2)\n",
    "    dtime_plate = pyro.plate(\"dtime\", T - 1, dim=-2)\n",
    "    place_plate = pyro.plate(\"place\", P, dim=-1)\n",
    "    time = torch.arange(float(T)) * 7 / 365.25  # in years\n",
    "    time -= time.max()\n",
    "\n",
    "    # Assume relative growth rate depends on kmer features but not time or place.\n",
    "    trend_coef = pyro.sample(\n",
    "        \"trend_coef\", dist.Laplace(0, feature_scale).expand([F]).to_event(1)\n",
    "    )\n",
    "    trend = pyro.deterministic(\"trend\", trend_coef @ features.T)\n",
    "\n",
    "    level = 0\n",
    "    if process_noise:\n",
    "        # Sample level from an improper uniform; we'll encode the prior as a factor graph.\n",
    "        with time_plate, place_plate:\n",
    "            rep = HaarReparam(dim=-3, experimental_allow_batch=True)\n",
    "            with poutine.reparam(config={\"level\": rep}):\n",
    "                level = pyro.sample(\n",
    "                    \"level\", dist.Normal(0, 1).expand([T, P, S]).to_event(1).mask(False)\n",
    "                )\n",
    "\n",
    "        # Add process noise depending on time, place, and strain, but with a shared scale.\n",
    "        noise_level = pyro.sample(\"noise_level\", dist.LogNormal(0, 1))\n",
    "        noise_scale = noise_level * level[1:].neg().exp().log1p().sqrt()\n",
    "        with dtime_plate, place_plate:\n",
    "            pyro.sample(\n",
    "                \"dynamics\",\n",
    "                dist.Normal(0, noise_scale).to_event(1),\n",
    "                obs=level[1:] - level[:-1],\n",
    "            )\n",
    "\n",
    "    # Finally observe overdispersed counts.\n",
    "    concentration = pyro.sample(\"concentration\", dist.LogNormal(2, 4))\n",
    "    strain_probs = (level + trend * time[:, None, None]).softmax(-1)\n",
    "    with time_plate, place_plate:\n",
    "        pyro.sample(\n",
    "            \"obs\",\n",
    "            dist.DirichletMultinomial(\n",
    "                total_count=weekly_strains.sum(-1).max(),\n",
    "                concentration=concentration * strain_probs,\n",
    "                is_sparse=True,  # uses a faster algorithm\n",
    "            ),\n",
    "            obs=weekly_strains,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "We'll use simple MAP estimation via Pyro's SVI and an `AutoDelta` guide. I'm defining a custom initialization function to initialize forecasts to uniformly even rather than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_loc_fn(site):\n",
    "    if site[\"name\"] in (\"trend_coef\", \"trend\", \"level\", \"level_haar\"):\n",
    "        return torch.zeros(site[\"fn\"].shape())\n",
    "    if site[\"name\"] == \"noise_level\":\n",
    "        return torch.full(site[\"fn\"].shape(), 0.1)\n",
    "    if site[\"name\"] == \"concentration\":\n",
    "        return torch.full(site[\"fn\"].shape(), 5.0)\n",
    "    return init_to_median(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(20210309)\n",
    "\n",
    "guide = AutoDelta(model, init_loc_fn=init_loc_fn)\n",
    "# Initialize guide so we can count parameters.\n",
    "guide(weekly_strains, weekly_infections, features)\n",
    "print(\"Traning guide with {} parameters:\".format(sum(p.numel() for p in guide.parameters())))\n",
    "optim = ClippedAdam({\"lr\": 0.05, \"betas\": (0.8, 0.99)})\n",
    "svi = SVI(model, guide, optim, Trace_ELBO())\n",
    "losses = []\n",
    "num_obs = weekly_strains.count_nonzero()\n",
    "for step in range(101):\n",
    "    loss = svi.step(weekly_strains, weekly_infections, features) / num_obs\n",
    "    assert not math.isnan(loss)\n",
    "    losses.append(loss)\n",
    "    if step % 20 == 0:\n",
    "        median = guide.median()\n",
    "        concentration = median[\"concentration\"].item()\n",
    "        print(f\"step {step: >4d} loss = {loss:0.3g}\\t\"\n",
    "              f\"conc. = {concentration:0.3g}\\t\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"SVI step\")\n",
    "plt.ylabel(\"loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(20210309)\n",
    "\n",
    "guide = AutoDelta(model, init_loc_fn=init_loc_fn)\n",
    "# Initialize guide so we can count parameters.\n",
    "guide(weekly_strains, weekly_infections, features)\n",
    "print(\"Traning guide with {} parameters:\".format(sum(p.numel() for p in guide.parameters())))\n",
    "optim = ClippedAdam({\"lr\": 0.05, \"betas\": (0.8, 0.99)})\n",
    "svi = SVI(model, guide, optim, Trace_ELBO())\n",
    "losses = []\n",
    "num_obs = weekly_strains.count_nonzero()\n",
    "for step in range(101):\n",
    "    loss = svi.step(weekly_strains, weekly_infections, features, process_noise=True) / num_obs\n",
    "    assert not math.isnan(loss)\n",
    "    losses.append(loss)\n",
    "    if step % 1 == 0:\n",
    "        median = guide.median()\n",
    "        concentration = median[\"concentration\"].item()\n",
    "        noise_level = median[\"noise_level\"].item()\n",
    "        print(f\"step {step: >4d} loss = {loss:0.3g}\\t\"\n",
    "              f\"conc. = {concentration:0.3g}\\t\"\n",
    "              f\"noise. = {noise_level:0.3g}\\t\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"SVI step\")\n",
    "plt.ylabel(\"loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we can extract a point estimate via the `guide.median()` method, which returns a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    median = {k: v.detach() for k, v in guide.median().items()}\n",
    "median[\"trend\"] = median[\"trend_coef\"] @ features.T\n",
    "print(\", \".join(median.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting results\n",
    "\n",
    "Since we're fitting relative growth rate and prevalence, we can characterize all lineages by these quantities. Note the relative transmissibility looks suspciously diverse, suggesting we should probably add process noise to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mutations = aa_features['mutations']\n",
    "xs, idx = median[\"trend_coef\"].sort(0)\n",
    "assert len(idx) == len(mutations)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"Regression coefficients (mutations)\")\n",
    "plt.plot(xs, 'k.', lw=0, markersize=0.5)\n",
    "plt.axhline(0, color='black', lw=0.5, alpha=0.5)\n",
    "plt.xlabel(\"mutation coefficient rank\")\n",
    "plt.ylabel(\"increased transmissibility\")\n",
    "\n",
    "I = len(idx)\n",
    "for i in range(20):\n",
    "    x = I / 10\n",
    "    y = xs.min() * (1 - i / 20)\n",
    "    plt.plot([i, x], [xs[i], y], color='blue', lw=0.5)\n",
    "    plt.text(x, y, \" \" + mutations[int(idx[i])], fontsize=5, color='blue',\n",
    "             verticalalignment=\"center\", horizontalalignment=\"left\")\n",
    "for i in range(I - 20, I):\n",
    "    x = I - I / 10\n",
    "    y = xs.max() * (1 - (I - i - 1) / 20)\n",
    "    plt.plot([i, x], [xs[i], y], color='red', lw=0.5)\n",
    "    plt.text(x, y, mutations[int(idx[i])] + \" \", fontsize=5, color='red',\n",
    "             verticalalignment=\"center\", horizontalalignment=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine coefficients of some known mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"N501Y\", \"D614G\"]:\n",
    "    i, = [i for i, m in enumerate(mutations) if name in m]\n",
    "    coef = float(median[\"trend_coef\"][i])\n",
    "    print(f\"{name} increases transmissibility by {coef:0.3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "edges = pangolin.find_edges(list(lineage_id))\n",
    "edges = torch.tensor([[lineage_id[u], lineage_id[v]] for u, v in edges], dtype=torch.long)\n",
    "\n",
    "def plot_prevalence(place_query=\"\", max_len=999):\n",
    "    ids = [i for name, i in location_id.items() if place_query in name]\n",
    "    local_lineages = weekly_strains.sum(0)[ids].sum(0).nonzero(as_tuple=True)[0]\n",
    "    assert ids, \"no locations matching \" + place_query\n",
    "    log_rate = median[\"trend\"] / 12\n",
    "    log_init = median[\"log_init\"][ids].logsumexp(0)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(log_rate[local_lineages], log_init[local_lineages], s=10, color=\"#ff7777\")\n",
    "    X, Y = [], []\n",
    "    for u, v in edges.tolist():\n",
    "        if u in local_lineages and v in local_lineages:\n",
    "            X.extend([log_rate[u], log_rate[v], None])\n",
    "            Y.extend([log_init[u], log_init[v], None])\n",
    "    plt.plot(X, Y, color=\"#ff7777\", lw=0.5)\n",
    "    for name, i in lineage_id.items():\n",
    "        plt.text(log_rate[i], log_init[i] + 0.08, name,\n",
    "                 fontsize=5, horizontalalignment=\"center\")\n",
    "    plt.ylabel(\"log prevalence\")\n",
    "    plt.xlabel(\"relative transmissibility (log monthly growth rate)\")\n",
    "    plt.title(f\"Prevalence and transmissibility of {len(local_lineages)} lineages\"\n",
    "              + (\" in \" + place_query if place_query else \" globally\"));\n",
    "    \n",
    "plot_prevalence(max_len=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prevalence(\"Massachusetts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use our logistic curves to forecast lineage prevalence in each region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decomposition(queries, num_parts=7, months_ahead=3):\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "    fig, axes = plt.subplots(len(queries), figsize=(8, 1 + 1.2 * len(queries)), sharex=True)\n",
    "    for row, (place_query, ax) in enumerate(zip(queries, axes)):\n",
    "        names = [name for name, i in location_id.items() if place_query in name]\n",
    "        ids = [location_id[name] for name in names]\n",
    "        assert ids, \"no locations matching \" + place_query\n",
    "        log_rate = median[\"log_rate\"]\n",
    "        # FIXME this ignores region population when aggregating:\n",
    "        log_init = median[\"log_init\"][ids].logsumexp(0)\n",
    "        assert log_init.shape == log_rate.shape\n",
    "        time = torch.linspace(0, months_ahead / 12.0, 100)\n",
    "        portion = (log_init + log_rate * time[:, None]).softmax(-1)\n",
    "\n",
    "        # Aggregate into top + others.\n",
    "        best = portion.sum(0).sort(0, descending=True).indices\n",
    "        parts = {\"other\": None}\n",
    "        for i in range(num_parts - 1):\n",
    "            i = best[num_parts - i - 2].item()\n",
    "            parts[lineage_id_inv[i]] = portion[:, i].clone()\n",
    "            portion[:, i] = 0\n",
    "        parts[\"other\"] = portion.sum(-1)\n",
    "        months = time * 12\n",
    "\n",
    "        ax.stackplot(months, *parts.values(), labels=tuple(parts))\n",
    "        ax.set_xlim(months.min(), months.max())\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_yticks(())\n",
    "        ax.set_ylabel(names[0].split(\"/\")[-1].strip() if len(names) == 1 else place_query)\n",
    "        if row == len(axes) - 1:\n",
    "            ax.set_xlabel(\"Lineage prevalence forecast (months in future)\")\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles[::-1], labels[::-1], loc=\"lower right\", prop={\"size\": 6.5})\n",
    "    plt.subplots_adjust(hspace=0.02);\n",
    "\n",
    "plot_decomposition([\"Mass\", \"Calif\", \"Texas\", \"Florida\", \"New York\", \"USA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
